{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculations on variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import iris\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '../outputs/stash_out/'\n",
    "\n",
    "files = {'fractional_cover' : 'vegcover1929oct.nc',\n",
    "         'alpha'            : 'alpha1929oct.nc',\n",
    "         'relative_humidity': 'relative_humidity1929oct.nc',\n",
    "         'lightning'        : 'lightning1929oct.nc'}\n",
    "\n",
    "tree =[101, 102]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {}\n",
    "for key, file in files.items():\n",
    "    data = iris.load_cube(dir + file)\n",
    "    input_data[key] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data['fractional_cover'].coord('pseudo_level').points==201 #| 202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bc179a.p52000jan.pp', 'bc179a.p52000feb.pp', 'bc179a.p52000mar.pp', 'bc179a.p52000apr.pp', 'bc179a.p52000may.pp', 'bc179a.p52000jun.pp', 'bc179a.p52000jul.pp', 'bc179a.p52000aug.pp', 'bc179a.p52000sep.pp', 'bc179a.p52000oct.pp', 'bc179a.p52000nov.pp', 'bc179a.p52000dec.pp']\n"
     ]
    }
   ],
   "source": [
    "# Loading in data from year 2000, ap5\n",
    "dir = '../data/UKESM/historic_1/'\n",
    "year = '2000'\n",
    "files = []\n",
    "months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "\n",
    "\n",
    "for month in months:\n",
    "    files.append('bc179a.p5' + year + month +'.pp')\n",
    " \n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some of Doug's code to help extract the correct layers of fractional cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m01s03i317 / (unknown)              (pseudo_level: 2; latitude: 144; longitude: 192)\n",
      "     Dimension coordinates:\n",
      "          pseudo_level                           x            -               -\n",
      "          latitude                               -            x               -\n",
      "          longitude                              -            -               x\n",
      "     Scalar coordinates:\n",
      "          forecast_period: 689400.0 hours, bound=(689040.0, 689760.0) hours\n",
      "          forecast_reference_time: 1850-01-01 00:00:00\n",
      "          time: 1929-10-16 00:00:00, bound=(1929-10-01 00:00:00, 1929-11-01 00:00:00)\n",
      "     Attributes:\n",
      "          STASH: m01s03i317\n",
      "          source: Data from Met Office Unified Model\n",
      "          um_version: 10.9\n",
      "     Cell methods:\n",
      "          mean: time (1 hour)\n"
     ]
    }
   ],
   "source": [
    "files = 'bc179a.p51929oct.pp'\n",
    "dir = '../data/'\n",
    "\n",
    "stash_constraint = iris.AttributeConstraint(STASH = \"m01s03i317\")\n",
    "cube = iris.load_cube(dir + files, stash_constraint)\n",
    "\n",
    "#index = np.where(cube.coord('pseudo_level').points)\n",
    "index = [cube.coord('pseudo_level').points == x for x in tree]\n",
    "index = np.any(index, axis = 0) # This combines all the boolean arrays together. True + False = True\n",
    "#print(index)\n",
    "#print(cube.coord('pseudo_level')[index])\n",
    "print(cube[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False False False]\n",
      "moisture_content_of_soil_layer / (kg m-2) (depth: 1; latitude: 144; longitude: 192)\n",
      "     Dimension coordinates:\n",
      "          depth                                 x            -               -\n",
      "          latitude                              -            x               -\n",
      "          longitude                             -            -               x\n",
      "     Scalar coordinates:\n",
      "          forecast_period: 689400.0 hours, bound=(689040.0, 689760.0) hours\n",
      "          forecast_reference_time: 1850-01-01 00:00:00\n",
      "          time: 1929-10-16 00:00:00, bound=(1929-10-01 00:00:00, 1929-11-01 00:00:00)\n",
      "     Attributes:\n",
      "          STASH: m01s08i223\n",
      "          source: Data from Met Office Unified Model\n",
      "          um_version: 10.9\n",
      "     Cell methods:\n",
      "          mean: time (1 hour)\n"
     ]
    }
   ],
   "source": [
    "files = 'bc179a.p51929oct.pp'\n",
    "dir = '../data/'\n",
    "\n",
    "stash_constraint = iris.AttributeConstraint(STASH = \"m01s08i223\")\n",
    "cube = iris.load_cube(dir + files, stash_constraint)\n",
    "\n",
    "#index = np.where(cube.coord('pseudo_level').points)\n",
    "index = [cube.coord('depth').points == 0.05]\n",
    "index = np.any(index, axis = 0) # This makes the cube happy\n",
    "print(cube[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to get alphaMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stash_constraint = iris.AttributeConstraint(STASH = 'm01s08i223')\n",
    "\n",
    "# Load all cubes\n",
    "aList =[]\n",
    "cube_list = iris.cube.CubeList()\n",
    "for f in files: \n",
    "    dat = iris.load_cube(dir + f, stash_constraint)\n",
    "    aList.append(dat)\n",
    "\n",
    "# Merge all cubes together\n",
    "cube_list = iris.cube.CubeList(aList)\n",
    "cube_alpha_new = cube_list.merge_cube() \n",
    "\n",
    "# Extract just the top soil\n",
    "index_soil = [cube_alpha_new.coord('depth').points == 0.05]\n",
    "index_soil = np.any(index_soil, axis = 0) # Still keep this in - it makes the cube happy\n",
    "cube_soil = cube_alpha_new[:, index_soil]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moisture_content_of_soil_layer / (kg m-2) (time: 6; depth: 1; latitude: 144; longitude: 192)\n",
      "     Dimension coordinates:\n",
      "          time                                 x         -            -               -\n",
      "          depth                                -         x            -               -\n",
      "          latitude                             -         -            x               -\n",
      "          longitude                            -         -            -               x\n",
      "     Auxiliary coordinates:\n",
      "          forecast_period                      x         -            -               -\n",
      "     Scalar coordinates:\n",
      "          forecast_reference_time: 1850-01-01 00:00:00\n",
      "     Attributes:\n",
      "          STASH: m01s08i223\n",
      "          source: Data from Met Office Unified Model\n",
      "          um_version: 10.9\n",
      "     Cell methods:\n",
      "          mean: time (1 hour)\n"
     ]
    }
   ],
   "source": [
    "print(cube_soil[6:,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rich's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8\n",
      "0\n",
      "0.13396860205608865\n",
      "0.13396860205608865\n",
      "1\n",
      "0.17919338060461956\n",
      "0.17919338060461956\n",
      "2\n",
      "0.22689228887143342\n",
      "0.22689228887143342\n",
      "3\n",
      "0.23411234980044157\n",
      "0.23411234980044157\n",
      "4\n",
      "0.23328306778617527\n",
      "0.23328306778617527\n",
      "5\n",
      "0.21472162993057914\n",
      "0.21472162993057914\n",
      "6\n",
      "0.19499083809230638\n",
      "0.19499083809230638\n",
      "7\n",
      "0.24470976124639096\n",
      "0.24470976124639096\n",
      "8\n",
      "0.2508361318837041\n",
      "0.2508361318837041\n"
     ]
    }
   ],
   "source": [
    "# Here, we're taking the 3rd time element all the way to the last time point and put it in a new cube\n",
    "# Note: we're only taking the first 3 months, because I've currently only uploaded a year's worth of data\n",
    "cube2 = cube_soil[3:,:,:,:]\n",
    "cube3 = cube_soil[3:,:,:,:]\n",
    "cube4 = cube_soil[3:,:,:,:]\n",
    "cube5 = cube_soil[3:,:,:,:]\n",
    "\n",
    "nmonths = len(cube2.coord(\"time\").points)\n",
    "print(*range(nmonths))\n",
    "\n",
    "# This loop is saying: for the first 3 months of the original cube (cube_soil), take those values, collapse them by the \n",
    "# mean, and then take the data and put it into cube2 as its first timepoint (which is 3 months ahead)\n",
    "\n",
    "for m in range( nmonths):\n",
    "    cube2.data[m,:,:,:] = cube_soil[m:m+3,:,:,:].collapsed([\"time\"], iris.analysis.MEAN).data\n",
    "    cube3.data[m,:,:,:] = cube_soil[m:m+3,:,:,:].collapsed([\"time\"], iris.analysis.MAX).data\n",
    "    cube4.data[m,:,:,:] = (cube3.data[m,:,:,:] / cube2.data[m,:,:,:]) - 1\n",
    "    cube5.data[m,:,:,:] = (cube3[m,:,:,:].data / cube2[m,:,:,:].data) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out to see if the values are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "cube_soil: 18.597834\n",
      "cube2: 18.59783403961416\n",
      "cube3: 19.65778791674471\n",
      "cube4: 1.1339685854704484\n",
      "1\n",
      "cube_soil: 18.266462\n",
      "cube2: 18.26646346460011\n",
      "cube3: 19.715636120996443\n",
      "cube4: 1.1791933806046195\n",
      "2\n",
      "cube_soil: 17.594904\n",
      "cube2: 17.594904535025286\n",
      "cube3: 19.598848098894923\n",
      "cube4: 1.2268923552139945\n",
      "3\n",
      "cube_soil: 16.533796\n",
      "cube2: 16.53379612286945\n",
      "cube3: 18.835416569114066\n",
      "cube4: 1.2341122834578804\n",
      "4\n",
      "cube_soil: 15.3931465\n",
      "cube2: 15.393147710245364\n",
      "cube3: 17.632717386214647\n",
      "cube4: 1.233283001443614\n",
      "5\n",
      "cube_soil: 14.438873\n",
      "cube2: 14.438872565087095\n",
      "cube3: 16.242471378067055\n",
      "cube4: 1.2147215470023778\n"
     ]
    }
   ],
   "source": [
    "for m in range(0,6):\n",
    "    print(m)\n",
    "    print('cube_soil: ' + str(cube_soil[m:m+3,:,:,:].collapsed(['time', 'depth', 'latitude', 'longitude'], iris.analysis.MEAN).data))\n",
    "    print('cube2: ' + str(cube2[m,:,:,:].collapsed(['time', 'depth', 'latitude', 'longitude'], iris.analysis.MEAN).data))\n",
    "    print('cube3: ' + str(cube3[m,:,:,:].collapsed(['time', 'depth', 'latitude', 'longitude'], iris.analysis.MEAN).data))\n",
    "    print('cube4: ' + str(cube4[m,:,:,:].collapsed(['time', 'depth', 'latitude', 'longitude'], iris.analysis.MEAN).data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.         2.5       ]\n",
      " [1.         1.66666667]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[5,5],[5,5]])\n",
    "y = np.array([[1,2], [5,3]])\n",
    "print(x/y)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
