{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width: 100%; border-collapse: collapse;\" border=\"0\">\n",
    "<tr>\n",
    "<td><b>Created:</b> Monday 30 January 2017</td>\n",
    "<td style=\"text-align: right;\"><a href=\"https://www.github.com/rhyswhitley/fire_limitation\">github.com/rhyswhitley/fire_limitation</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<div>\n",
    "<center>\n",
    "<font face=\"Times\">\n",
    "<br>\n",
    "<h1>Quantifying the uncertainity of a global fire limitation model using Bayesian inference</h1>\n",
    "<h2>Part 1: Staging data for analysis</h2>\n",
    "<br>\n",
    "<br>\n",
    "<sup>1,* </sup>Douglas Kelley, \n",
    "<sup>2 </sup>Ioannis Bistinas, \n",
    "<sup>3, 4 </sup>Chantelle Burton, \n",
    "<sup>1 </sup>Tobias Marthews, \n",
    "<sup>5 </sup>Rhys Whitley\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<sup>1 </sup>Centre for Ecology and Hydrology, Maclean Building, Crowmarsh Gifford, Wallingford, Oxfordshire, United Kingdom\n",
    "<br>\n",
    "<sup>2 </sup>Vrije Universiteit Amsterdam, Faculty of Earth and Life Sciences, Amsterdam, Netherlands\n",
    "<br>\n",
    "<sup>3 </sup>Met Office United Kingdom, Exeter, United Kingdom\n",
    "<br>\n",
    "<sup>4 </sup>Geography, University of Exeter, Exeter, United Kingdom\n",
    "<br>\n",
    "<sup>5 </sup>Natural Perils Pricing, Commercial & Consumer Portfolio & Pricing, Suncorp Group, Sydney, Australia\n",
    "<br>\n",
    "<br>\n",
    "<h3>Summary</h3>\n",
    "<hr>\n",
    "<p> \n",
    "This notebook aims to process the separate netCDF4 files for the model drivers (X<sub>i=1, 2, ... M</sub>) and model target (Y) into a unified tabular data frame, exported as a compressed comma separated value (CSV) file. This file is subsequently used in the Bayesian inference study that forms the second notebook in this experiment. The advantage of the pre-processing the data separately to the analysis allows for it be quickly staged on demand. Of course other file formats may be more advantageous for greater compression (e.g. SQLite3 database file).\n",
    "</p>\n",
    "<br>\n",
    "<b>You will need to run this notebook to prepare the dataest before you attempt the Bayesian analysis in Part 2</b>.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<i>Python code and calculations below</i>\n",
    "<br>\n",
    "<hr>\n",
    "</font>\n",
    "</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data munging and analytical libraries \n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset \n",
    "\n",
    "# graphical libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# set paths\n",
    "inPath = \"../data/UKESM/retrieved_codes/\"\n",
    "outPath = \"../data/globfire_2000.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and clean data\n",
    "\n",
    "Set the directory path and look for all netcdf files that correspond to the model drivers and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/UKESM/retrieved_codes/alpha2000.nc</td>\n",
       "      <td>alpha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/UKESM/retrieved_codes/cropland2000.nc</td>\n",
       "      <td>cropland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/UKESM/retrieved_codes/vegcover2000.nc</td>\n",
       "      <td>vegcover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/UKESM/retrieved_codes/alphaMax2000.nc</td>\n",
       "      <td>alphaMax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/UKESM/retrieved_codes/treecover2000.nc</td>\n",
       "      <td>treecover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>../data/UKESM/retrieved_codes/relative_humidit...</td>\n",
       "      <td>relative_humidity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>../data/UKESM/retrieved_codes/pasture2000.nc</td>\n",
       "      <td>pasture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>../data/UKESM/retrieved_codes/lightning2000.nc</td>\n",
       "      <td>lightning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filepath          file_name\n",
       "0         ../data/UKESM/retrieved_codes/alpha2000.nc              alpha\n",
       "1      ../data/UKESM/retrieved_codes/cropland2000.nc           cropland\n",
       "2      ../data/UKESM/retrieved_codes/vegcover2000.nc           vegcover\n",
       "3      ../data/UKESM/retrieved_codes/alphaMax2000.nc           alphaMax\n",
       "4     ../data/UKESM/retrieved_codes/treecover2000.nc          treecover\n",
       "5  ../data/UKESM/retrieved_codes/relative_humidit...  relative_humidity\n",
       "6       ../data/UKESM/retrieved_codes/pasture2000.nc            pasture\n",
       "7     ../data/UKESM/retrieved_codes/lightning2000.nc          lightning"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! mv emc2000-2014_masked.nc ../\n",
    "driver_paths = [os.path.join(dp, f) for (dp, _, fn) in os.walk(inPath) for f in fn if f.endswith('.nc')]\n",
    "driver_names = [re.search('^[a-zA-Z_]*', os.path.basename(fp)).group(0) for fp in driver_paths]\n",
    "\n",
    "file_table = pd.DataFrame({'filepath': driver_paths, 'file_name': driver_names})\n",
    "file_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to extract the variable values from each netCDF4 file. Variables are flattened from a 3 dimensional array to 1 dimensional version, pooling all values both spatially and temporily. \n",
    "\n",
    "Don't know if this is the correct way to do this, but will come back to it once I understand the model (and its optimisation) better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nc_extract(fpath):\n",
    "    print(\"Processing: {0}\".format(fpath))\n",
    "    with Dataset(fpath, 'r') as nc_file:\n",
    "        gdata = np.array(nc_file.variables['variable'][:,:,:])\n",
    "        gdata[gdata < -9E9] = np.nan\n",
    "        gflat = gdata.flatten()\n",
    "        if type(gdata) == np.ma.core.MaskedArray:\n",
    "            return gflat[~gflat.mask].data\n",
    "        else:\n",
    "            return gflat.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the above function on all netCDF4 file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ../data/UKESM/retrieved_codes/alpha2000.nc\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'variable'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bda4b53cc56e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnc_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdriver_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-bda4b53cc56e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnc_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdriver_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-0e2cd1c283d7>\u001b[0m in \u001b[0;36mnc_extract\u001b[0;34m(fpath)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnc_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mgdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnc_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'variable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mgdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgdata\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m9E9\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mgflat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'variable'"
     ]
    }
   ],
   "source": [
    "values = [nc_extract(dp) for dp in driver_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn this into a dataframe for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4644864, 10)\n",
      "10\n",
      "['alpha_', 'population_density', 'pasture', 'lightning_ignitions', 'relative_humidity', 'alpha', 'fire', 'treecover', 'cropland', 'vegcover']\n",
      "(4644864, 10)\n"
     ]
    }
   ],
   "source": [
    "# turn list into a dataframe\n",
    "print(np.array(values).T.shape)\n",
    "print(len(driver_names))\n",
    "print(driver_names)\n",
    "\n",
    "fire_df = pd.DataFrame(np.array(values).T, columns=driver_names)\n",
    "print(fire_df.shape)\n",
    "##fire_df.info()\n",
    "##fire_df.columns = driver_names\n",
    "# replace null flags with pandas null\n",
    "#fire_df.replace(fire_df < -3e38, np.nan, inplace=TruTe)\n",
    "#fire_df[] = np.nan\n",
    "# drop all null rows (are ocean and not needed in optim)\n",
    "\n",
    "##fire_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we've built it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha_</th>\n",
       "      <th>population_density</th>\n",
       "      <th>pasture</th>\n",
       "      <th>lightning_ignitions</th>\n",
       "      <th>relative_humidity</th>\n",
       "      <th>alpha</th>\n",
       "      <th>fire</th>\n",
       "      <th>treecover</th>\n",
       "      <th>cropland</th>\n",
       "      <th>vegcover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "      <td>9.969210e+36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         alpha_  population_density       pasture  lightning_ignitions  \\\n",
       "0  9.969210e+36        9.969210e+36  9.969210e+36         9.969210e+36   \n",
       "1  9.969210e+36        9.969210e+36  9.969210e+36         9.969210e+36   \n",
       "2  9.969210e+36        9.969210e+36  9.969210e+36         9.969210e+36   \n",
       "3  9.969210e+36        9.969210e+36  9.969210e+36         9.969210e+36   \n",
       "4  9.969210e+36        9.969210e+36  9.969210e+36         9.969210e+36   \n",
       "\n",
       "   relative_humidity         alpha  fire     treecover      cropland  \\\n",
       "0       9.969210e+36  9.969210e+36   0.0  9.969210e+36  9.969210e+36   \n",
       "1       9.969210e+36  9.969210e+36   0.0  9.969210e+36  9.969210e+36   \n",
       "2       9.969210e+36  9.969210e+36   0.0  9.969210e+36  9.969210e+36   \n",
       "3       9.969210e+36  9.969210e+36   0.0  9.969210e+36  9.969210e+36   \n",
       "4       9.969210e+36  9.969210e+36   0.0  9.969210e+36  9.969210e+36   \n",
       "\n",
       "       vegcover  \n",
       "0  9.969210e+36  \n",
       "1  9.969210e+36  \n",
       "2  9.969210e+36  \n",
       "3  9.969210e+36  \n",
       "4  9.969210e+36  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire_df.head()\n",
    "# fire_df[fire_df['fire']>0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export this to disk to be used by the analysis notebook - used gzip compression to save on space. Beware, because of there are approximation 10 million rows of data, this may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = os.path.expanduser(outPath)\n",
    "fire_df.to_csv(savepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<font size=\"5\">\n",
    "<a style=\"font-weight: bold; size: 5\" href=\"http://localhost:8888/notebooks/notebooks/bayesian_inference.ipynb\">Part 2: click here</a>\n",
    "</font>\n",
    "</center>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
