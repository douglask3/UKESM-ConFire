{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving input from UK-ESM: mulitple years\n",
    "\n",
    "[23/09/19]\n",
    "\n",
    "This is to be run on JASMIN and points to the revelent directories in that working space, as well as if it is being run from the notebooks folder of the conFIRE repository. For a more detailed walk through, go to retrieve_stash (retrieving for single years).\n",
    "\n",
    "This extraction doesn't take into account canopy; ``treecover`` will still need to be multiplied by 0.8.\n",
    "``lightning`` units are adjusted to \"number of strikes to hit the ground\" or something like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import iris\n",
    "import iris.coord_categorisation\n",
    "import matplotlib.pyplot as plt\n",
    "import iris.plot as iplt\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Not sure these are needed\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# from   libs.plot_maps    import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/gws/nopw/j04/cmip6_prep_vol1/ukesm1-initial/CMIP6/CMIP/UKESM1-0-LL/historical/r1i1p1f2/round-1-monthly/input/u-bc179/ap5/'\n",
    "outfile = '../data/retrieved_codes/2000-2014/'\n",
    "\n",
    "# This is the location of soil porosity\n",
    "dir_poro = '../data/'\n",
    "\n",
    "# The year ranges that you want\n",
    "years = range(1999,2015)\n",
    "\n",
    "months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "files = []\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        files.append('bc179a.p5' + str(year) + month +'.pp')\n",
    "        \n",
    "        \n",
    "d = 12 # 12 # The number of months to skip for alphaMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stash_conFIRE = {'vegcover'           : 'm01s03i317',\n",
    "                 'alpha'              : 'm01s08i223',\n",
    "                 'lightning'          : 'm01s50i082',\n",
    "#                'population_density' : 'population_density2000-2014.nc',\n",
    "                 'relative_humidity'  : 'm01s03i245'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable location:\n",
    "\n",
    "### ap5\n",
    "\n",
    "* ``vegcover``\n",
    "* ``treecover``\n",
    "* ``alpha``\n",
    "* ``relative_humdity``\n",
    "* ``lightning``\n",
    "* `pasture`\n",
    "* `cropland`\n",
    "\n",
    "Everything appears to be in apt5, so we're going to focus on just apt5 files for now. The fractional cover (`'m01s03i317'`) consists of 27 different vegetation types. Below is a key to identify the coordinate codes (these can be viewed with `cube.coord('psuedo_level').points`). Similar for alpha, we only want the top level of moisture, so just the first layer has to be extracted.\n",
    "\n",
    "* `treecover`\n",
    "   * 101 = Broadlead deciduous tree\n",
    "   * 102 = Broadlead evergreen tree   \n",
    "   * 103  =  Broadlead  temperate evergreen tree\n",
    "   * 201 = Needleleaf deciduous    \n",
    "   * 202 = Needleleaf evergreen  \n",
    "* `cropland`\n",
    " * 301 = Grass C3 crop    \n",
    " * 401 = Grass C4 crop    \n",
    "* `pasture`\n",
    " * 302 = Grass C3 pature    \n",
    " * 402 = Grass C4 pasture\n",
    "* `vegcover` (This includes the list below and all of the above):\n",
    " * 501 = Shrub decidious    \n",
    " * 502 = Shrub evergreen    \n",
    " * 3 = Grass C3 natural\n",
    " * 4 = Grass C4 natural  \n",
    "   \n",
    "_Note: You'll need to alter all the output cubes to skip the first year of data (in order to make them the same length as alphaMax). You'll also need to set ``d`` to specify the number of months to skip. Lines in the script that make this change are labelled #xxx._\n",
    "\n",
    "### Extracting variables from the files\n",
    "\n",
    "[10/09/19]: I've taken out the scaling of ``RH`` to turn it into a probability as I think this is already done in UKESM.\n",
    "\n",
    "[23/09/19]: Adjusted the ``lightning`` units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treeCover = [101, 102, 103, 201, 202]\n",
    "cropland = [301, 401]\n",
    "pasture = [302, 402]\n",
    "vegcover = treeCover + cropland + pasture + [3, 4, 501, 502]\n",
    "\n",
    "name_codes = [treeCover, cropland, pasture, vegcover]\n",
    "name = ['treeCover', 'cropland', 'pasture', 'vegcover']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaMax and alpha\n",
    "\n",
    "To create alphaMax, the maximum alpha of the previous 12 months must be divided by the mean alpha of the last 12 months and then 1 must be subtracted from the result:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\alpha_{max}}{\\alpha} -1\n",
    "\\end{equation}\n",
    "\n",
    "#### Calculating alphaMax\n",
    "\n",
    "_Note: when running this on all historical data, the first year must be neglected in order to find alphaMax._\n",
    "\n",
    "The next section takes the first x (soon to be 12) months of the original cube (cube_soil) and collapses by the mean (cube2) and max (cube3). The alphaMax calculation is then done and the results of which are saved in the alphaMax cube. Note this will have -x timepoints to all the other variables so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stash_conFIRE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-95e64039d16a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstash_conFIRE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m# Extracting lightning and relative_humidity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lightning'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'relative_humidity'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mstash_constraint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttributeConstraint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTASH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstash_conFIRE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stash_conFIRE' is not defined"
     ]
    }
   ],
   "source": [
    "for l in stash_conFIRE.keys():\n",
    "    \n",
    "        # Extracting lightning and relative_humidity\n",
    "    if l == 'lightning' or l == 'relative_humidity':\n",
    "        stash_constraint = iris.AttributeConstraint(STASH = stash_conFIRE[l])\n",
    "        print('Now loading: ' + l)\n",
    "\n",
    "        # Load all cubes\n",
    "        aList =[]\n",
    "        cube_list = iris.cube.CubeList()\n",
    "        for f in files:\n",
    "            dat = iris.load_cube(dir + f, stash_constraint)\n",
    "            aList.append(dat)\n",
    "        #    print(str(f) + ' file loaded')\n",
    "\n",
    "        # Merge all cubes together\n",
    "        cube_list = iris.cube.CubeList(aList)\n",
    "        cubes = cube_list.merge_cube()\n",
    "\n",
    "        # Changing units (RH)\n",
    "        if l == 'relative_humidity':\n",
    "            cubes.convert_units(1)\n",
    "            time = len(cubes.coord(\"time\").points)\n",
    "            # For adjusting RH to a probability (not needed, I think)\n",
    "#             for t in range(time):\n",
    "#                 cubes.data[t,:,:] = cubes.data[t,:,:] / 100\n",
    "\n",
    "            print('Range of relative humdity: ' + str(cubes.data.min) + '-' + str(cubes.data.max))\n",
    "\n",
    "        # Changing units (lightning)\n",
    "        if l == 'lightning':\n",
    "            F = cubes\n",
    "            cubes_F = cubes\n",
    "\n",
    "            F.data = (cubes.data * 1000000)**(0.4180) * 0.0408\n",
    "            F.data[F.data > 1] = 1\n",
    "            F.data[cubes.data == 0] = 0\n",
    "            cubes_F.data = F.data * cubes.data\n",
    "            cubes_F = cubes_F[d:,:,:]\n",
    "\n",
    "            print(type(cubes_F))\n",
    "\n",
    "        # For skipping the first x months\n",
    "        #xxx\n",
    "\n",
    "        out = outfile + l + str(years[1]) + '-' + str(years[len(years)-1]) + '.nc'\n",
    "        if l == 'lightning':\n",
    "            iris.save(cubes_F, out)\n",
    "        else:\n",
    "            cubes = cubes[d:,:,:]\n",
    "            iris.save(cubes, out)\n",
    "        print(l + ' has been saved')\n",
    "\n",
    "        \n",
    "    # For vegcover, treecover, pasture and cropland\n",
    "    elif l == 'vegcover':\n",
    "        stash_constraint = iris.AttributeConstraint(STASH = stash_conFIRE[l])\n",
    "        print('Now loading: ' + l)\n",
    "\n",
    "        # Load all cubes\n",
    "        aList =[]\n",
    "        cube_list = iris.cube.CubeList()\n",
    "        for f in files: \n",
    "            dat = iris.load_cube(dir + f, stash_constraint)\n",
    "            aList.append(dat)\n",
    "        #    print(str(f) + ' file loaded')\n",
    "\n",
    "        # Merge all cubes together\n",
    "        cube_list = iris.cube.CubeList(aList)\n",
    "        cube_fractional = cube_list.merge_cube() \n",
    "\n",
    "\n",
    "        for var_type in range(0,len(name_codes)):\n",
    "            index = [cube_fractional.coord('pseudo_level').points == x  for x in name_codes[var_type]]\n",
    "\n",
    "            # This combines all the boolean arrays together. True + False = True\n",
    "            index = np.any(index, axis = 0)\n",
    "            print('Indices for ' + name[var_type])\n",
    "            #print(index)\n",
    "\n",
    "            # Extracts just the layers we want and saves\n",
    "            cube = cube_fractional[:,index]\n",
    "\n",
    "            # For skipping the first x months\n",
    "            #xxx\n",
    "            cube = cube[d:,:,:,:].collapsed(['pseudo_level'], iris.analysis.SUM)\n",
    "\n",
    "            out = outfile + name[var_type] + str(years[0]) + '-' + str(years[len(years)-1]) + '.nc'\n",
    "            iris.save(cube, out)\n",
    "            print(name[var_type] + ' has been saved')\n",
    "            \n",
    "            \n",
    "    # For alpha & alphaMax        \n",
    "    elif l == 'alpha':\n",
    "        stash_constraint = iris.AttributeConstraint(STASH = stash_conFIRE[l])\n",
    "        print('Now loading: ' + l)\n",
    "\n",
    "        # Load all cubes\n",
    "        aList =[]\n",
    "        cube_list = iris.cube.CubeList()\n",
    "        for f in files: \n",
    "            dat = iris.load_cube(dir + f, stash_constraint)\n",
    "            aList.append(dat)\n",
    "        #    print(str(f) + ' has loaded')\n",
    "\n",
    "        # Merge all cubes together\n",
    "        cube_list = iris.cube.CubeList(aList)\n",
    "        cube_alpha = cube_list.merge_cube() \n",
    "\n",
    "        # Extract just the top soil\n",
    "        index_soil = [cube_alpha.coord('depth').points == 0.05]\n",
    "        index_soil = np.any(index_soil, axis = 0) # Still keep this in - it makes the cube happy\n",
    "        cube_soil = cube_alpha[:, index_soil]\n",
    "        cube_soil = cube_soil[:,0,:,:]\n",
    "        cube_soil.long_name = 'alpha'\n",
    "\n",
    "\n",
    "        # Turning soil moisture into alpha: alpha = soil_moisture * soil_porosity * 1.2 (to scale it) / 50 (convert units)\n",
    "        porosity = iris.load(dir_poro + 'qrparm.soil.nc')[5] # 5 = soil porosity\n",
    "        time = len(cube_soil.coord(\"time\").points)\n",
    "        for t in range(time):\n",
    "            cube_soil.data[t,:,:] = cube_soil.data[t,:,:] * porosity.data * 1.2 / 50\n",
    "\n",
    "        \n",
    "        #xxx\n",
    "        cube_soil_skip_year = cube_soil[d:,:,:]\n",
    "\n",
    "        # Save alpha\n",
    "        out = outfile + cube_soil.long_name + str(years[0]) + '-' + str(years[len(years)-1]) + '.nc'\n",
    "        iris.save(cube_soil_skip_year, out)\n",
    "        print(str(l) + ' has been saved')\n",
    "\n",
    "        # Calculating alphaMax\n",
    "        #xxx\n",
    "        cube2 = cube_soil[d:,:,:]\n",
    "        cube3 = cube_soil[d:,:,:]\n",
    "        alphaMax = cube_soil[d:,:,:]\n",
    "\n",
    "        nmonths = len(cube2.coord(\"time\").points)\n",
    "\n",
    "        #xxx\n",
    "        for m in range( nmonths):\n",
    "            cube2.data[m,:,:] = cube_soil[m:m+d,:,:].collapsed([\"time\"], iris.analysis.MEAN).data\n",
    "            cube3.data[m,:,:] = cube_soil[m:m+d,:,:].collapsed([\"time\"], iris.analysis.MAX).data\n",
    "            alphaMax.data[m,:,:] = (cube3.data[m,:,:] / cube2.data[m,:,:]) - 1\n",
    "\n",
    "\n",
    "        # Saving alphaMax\n",
    "        alphaMax.long_name = 'alphaMax'\n",
    "        out = outfile + alphaMax.long_name + str(years[0]) + '-' + str(years[len(years)-1]) + '.nc'\n",
    "        iris.save(alphaMax, out)\n",
    "        print(alphaMax.long_name + ' has been saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
