{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving input from UK-ESM: mulitple years\n",
    "\n",
    "This is to be run on JASMIN and points to the revelent directories in that working space. For a more detailed walk through, go to retrieve_stash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import iris\n",
    "import iris.coord_categorisation\n",
    "import matplotlib.pyplot as plt\n",
    "import iris.plot as iplt\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Not sure these are needed\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# from   libs.plot_maps    import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/home/users/mbrown/UKESM/u-bc179/ap5/'\n",
    "dir_poro = '/home/users/mbrown/'\n",
    "outfile = '/home/users/mbrown/outputs/'\n",
    "\n",
    "# The year ranges that you want\n",
    "years = range(2000,2015)\n",
    "\n",
    "months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "files = []\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        files.append('bc179a.p5' + str(year) + month +'.pp')\n",
    "        \n",
    "        \n",
    "d = 12 # 12 # The number of months to skip for alphaMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stash_conFIRE = {'vegcover'           : 'm01s03i317',\n",
    "                 'alpha'              : 'm01s08i223',\n",
    "                 'lightning'          : 'm01s50i082',\n",
    "#                'population_density' : 'population_density2000-2014.nc',\n",
    "                 'relative_humidity'  : 'm01s03i245'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treeCover = [101, 102, 103, 201, 202]\n",
    "cropland = [301, 401]\n",
    "pasture = [302, 402]\n",
    "vegcover = treeCover + cropland + pasture + [3, 4, 501, 502]\n",
    "\n",
    "name_codes = [treeCover, cropland, pasture, vegcover]\n",
    "name = ['treeCover', 'cropland', 'pasture', 'vegcover']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting variables from the files\n",
    "\n",
    "I'm going to try and condense this down into one cell on Jupter notebooks (it may not work though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in stash_conFIRE.keys():\n",
    "    \n",
    "    # Extracting lightning and relative_humidity\n",
    "    if l == 'lightning' or l == 'relative_humidity':\n",
    "        stash_constraint = iris.AttributeConstraint(STASH = stash_conFIRE[l])\n",
    "        print('Now loading: ' + l)\n",
    "\n",
    "        # Load all cubes\n",
    "        aList =[]\n",
    "        cube_list = iris.cube.CubeList()\n",
    "        for f in files: \n",
    "            dat = iris.load_cube(dir + f, stash_constraint)\n",
    "            aList.append(dat)\n",
    "        #    print(str(f) + ' file loaded')\n",
    "\n",
    "        # Merge all cubes together\n",
    "        cube_list = iris.cube.CubeList(aList)\n",
    "        cubes = cube_list.merge_cube()\n",
    "\n",
    "        # For skipping the first x months\n",
    "        #xxx\n",
    "        cubes = cubes[d:,:,:]\n",
    "        \n",
    "        if l == 'relative_humidity':\n",
    "            time = len(cubes.coord(\"time\").points)\n",
    "            for t in range(time):\n",
    "                cubes.data[t,:,:] = cubes.data[t,:,:] / 100\n",
    "\n",
    "        print(l + ' has been saved')\n",
    "        out = outfile + l + str(years[0]) + '-' + str(years[len(years)-1]) + '.nc'\n",
    "        iris.save(cubes, out)\n",
    "        \n",
    "        \n",
    "    # For vegcover, treecover, pasture and cropland\n",
    "    elif l == 'vegcover':\n",
    "        stash_constraint = iris.AttributeConstraint(STASH = stash_conFIRE[l])\n",
    "        print('Now loading: ' + l)\n",
    "\n",
    "        # Load all cubes\n",
    "        aList =[]\n",
    "        cube_list = iris.cube.CubeList()\n",
    "        for f in files: \n",
    "            dat = iris.load_cube(dir + f, stash_constraint)\n",
    "            aList.append(dat)\n",
    "        #    print(str(f) + ' file loaded')\n",
    "\n",
    "        # Merge all cubes together\n",
    "        cube_list = iris.cube.CubeList(aList)\n",
    "        cube_fractional = cube_list.merge_cube() \n",
    "\n",
    "\n",
    "        for var_type in range(0,len(name_codes)):\n",
    "            index = [cube_fractional.coord('pseudo_level').points == x  for x in name_codes[var_type]]\n",
    "\n",
    "            # This combines all the boolean arrays together. True + False = True\n",
    "            index = np.any(index, axis = 0)\n",
    "            print('Indices for ' + name[var_type])\n",
    "            #print(index)\n",
    "\n",
    "            # Extracts just the layers we want and saves\n",
    "            cube = cube_fractional[:,index]\n",
    "\n",
    "            # For skipping the first x months\n",
    "            #xxx\n",
    "            cube = cube[d:,:,:,:].collapsed(['pseudo_level'], iris.analysis.SUM)\n",
    "\n",
    "            out = outfile + name[var_type] + str(years[0]) + '-' + str(years[len(years)-1]) + '.nc'\n",
    "            iris.save(cube, out)\n",
    "            print(name[var_type] + ' has been saved')\n",
    "            \n",
    "            \n",
    "    # For alpha & alphaMax        \n",
    "    elif l == 'alpha':\n",
    "        stash_constraint = iris.AttributeConstraint(STASH = stash_conFIRE[l])\n",
    "        print('Now loading: ' + l)\n",
    "\n",
    "        # Load all cubes\n",
    "        aList =[]\n",
    "        cube_list = iris.cube.CubeList()\n",
    "        for f in files: \n",
    "            dat = iris.load_cube(dir + f, stash_constraint)\n",
    "            aList.append(dat)\n",
    "        #    print(str(f) + ' has loaded')\n",
    "\n",
    "        # Merge all cubes together\n",
    "        cube_list = iris.cube.CubeList(aList)\n",
    "        cube_alpha = cube_list.merge_cube() \n",
    "\n",
    "        # Extract just the top soil\n",
    "        index_soil = [cube_alpha.coord('depth').points == 0.05]\n",
    "        index_soil = np.any(index_soil, axis = 0) # Still keep this in - it makes the cube happy\n",
    "        cube_soil = cube_alpha[:, index_soil]\n",
    "        cube_soil = cube_soil[:,0,:,:]\n",
    "        cube_soil.long_name = 'alpha'\n",
    "\n",
    "\n",
    "        # Turning soil moisture into alpha: alpha = soil_moisture * soil_porosity * 1.2 (to scale it) / 50 (convert units)\n",
    "        porosity = iris.load(dir_poro + 'qrparm.soil.nc')[5] # 5 = soil porosity\n",
    "        time = len(cube_soil.coord(\"time\").points)\n",
    "        for t in range(time):\n",
    "            cube_soil.data[t,:,:] = cube_soil.data[t,:,:] * porosity.data * 1.2 / 50\n",
    "\n",
    "        \n",
    "        #xxx\n",
    "        cube_soil_skip_year = cube_soil[d:,:,:]\n",
    "\n",
    "        # Save alpha\n",
    "        out = outfile + cube_soil.long_name + str(years[0]) + '-' + str(years[len(years)-1]) + '.nc'\n",
    "        iris.save(cube_soil_skip_year, out)\n",
    "        print(str(l) + ' has been saved')\n",
    "\n",
    "        # Calculating alphaMax\n",
    "        #xxx\n",
    "        cube2 = cube_soil[d:,:,:]\n",
    "        cube3 = cube_soil[d:,:,:]\n",
    "        alphaMax = cube_soil[d:,:,:]\n",
    "\n",
    "        nmonths = len(cube2.coord(\"time\").points)\n",
    "\n",
    "        #xxx\n",
    "        for m in range( nmonths):\n",
    "            cube2.data[m,:,:] = cube_soil[m:m+d,:,:].collapsed([\"time\"], iris.analysis.MEAN).data\n",
    "            cube3.data[m,:,:] = cube_soil[m:m+d,:,:].collapsed([\"time\"], iris.analysis.MAX).data\n",
    "            alphaMax.data[m,:,:] = (cube3.data[m,:,:] / cube2.data[m,:,:]) - 1\n",
    "\n",
    "\n",
    "        # Saving alphaMax\n",
    "        alphaMax.long_name = 'alphaMax'\n",
    "        out = outfile + alphaMax.long_name + str(years[0]) + '-' + str(years[len(years)-1]) + '.nc'\n",
    "        iris.save(alphaMax, out)\n",
    "        print(alphaMax.long_name + ' has been saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
