{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stash codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember: use ceh conda environment\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import iris\n",
    "import iris.coord_categorisation\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set variables\n",
    "\n",
    "For now, I'm removing the variables which I can't find in either apt4 or apt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bc179a.p52000jan.pp', 'bc179a.p52000feb.pp', 'bc179a.p52000mar.pp', 'bc179a.p52000apr.pp', 'bc179a.p52000may.pp', 'bc179a.p52000jun.pp', 'bc179a.p52000jul.pp', 'bc179a.p52000aug.pp', 'bc179a.p52000sep.pp', 'bc179a.p52000oct.pp', 'bc179a.p52000nov.pp', 'bc179a.p52000dec.pp']\n"
     ]
    }
   ],
   "source": [
    "# Loading in data from year 2000, ap5\n",
    "dir = '../data/UKESM/historic_1/'\n",
    "outfile = '../data/UKESM/retrieved_codes/'\n",
    "year = '2000'\n",
    "months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "files = []\n",
    "\n",
    "for month in months:\n",
    "    files.append('bc179a.p5' + year + month +'.pp')\n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir1 = '../data/'\n",
    "file = 'bc179a.p41930oct.pp'\n",
    "file2 = 'bc179a.p51929oct.pp'\n",
    "file3 = 'bc179a.p41929oct.pp'\n",
    "\n",
    "stash_conFIRE = {'vegcover'           : 'm01s03i317',\n",
    "                 'alpha'              : 'm01s08i223',\n",
    "#                'emc'                : 'm01s03i245',\n",
    "#                'treeCover'          : 'm01s03i317', # same as vegcover\n",
    "                 'lightning'          : 'm01s50i082',\n",
    "#                'pasture'            : 'm01s00i458',\n",
    "#                'population_density' : 'population_density2000-2014.nc',\n",
    "                 'relative_humidity'  : 'm01s03i245'}\n",
    "#                'cropland'           : 'm01s00i448'}\n",
    "\n",
    "# cube list\n",
    "list_of_stash_ap4 = []\n",
    "list_of_stash_ap5 = []\n",
    "\n",
    "#cl_ap4 = iris.load(dir1 + file3)\n",
    "#cl_ap5 = iris.load(dir1 + file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking to see if STASH codes match any in the file\n",
    "Uncomment last two lines in previous cell to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vegcover in ap5\n",
      "lightning in ap5\n",
      "alpha in ap5\n",
      "relative_humidity in ap5\n"
     ]
    }
   ],
   "source": [
    "#print(c.attributes['STASH'])\n",
    "\n",
    "i = 0\n",
    "for c5 in cl_ap5:\n",
    "    c5 = cl_ap5[i]\n",
    "    for name, dat in stash_conFIRE.items():\n",
    "        if c5.attributes[\"STASH\"] == dat:\n",
    "            print(name + \" in ap5\")\n",
    "    i += 1\n",
    "\n",
    "j = 0\n",
    "for c4 in cl_ap4:\n",
    "    c4 = cl_ap4[j]\n",
    "    for name, dat in stash_conFIRE.items():\n",
    "        if c4.attributes[\"STASH\"] == dat:\n",
    "            print(name + \" in ap4\")\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable location:\n",
    "\n",
    "### ap5\n",
    "\n",
    "* ``vegcover``\n",
    "* ``treecover``\n",
    "* ``alpha``\n",
    "* ``relative_humdity``\n",
    "* ``lightning``\n",
    "* `pasture`\n",
    "* `cropland`\n",
    "\n",
    "Everything appears to be in apt5, so we're going to focus on just apt5 files for now. The fractional cover (`'m01s03i317'`) consists of 27 different vegetation types. Below is a key to identify the coordinate codes (these can be viewed with `cube.coord('psuedo_level').points`. Similar for alpha, we only want the top level of moisture, so just the first layer has to be extracted.\n",
    "\n",
    "* `treecover`\n",
    "   * 101 = Broadlead deciduous tree\n",
    "   * 102 = Broadlead evergreen tree   \n",
    "   * 103  =  Broadlead  temperate evergreen tree\n",
    "   * 201 = Needleleaf deciduous    \n",
    "   * 202 = Needleleaf evergreen  \n",
    "* `cropland`\n",
    " * 301 = Grass C3 crop    \n",
    " * 401 = Grass C4 crop    \n",
    "* `pasture`\n",
    " * 302 = Grass C3 pature    \n",
    " * 402 = Grass C4 pasture\n",
    "* `vegcover` (This includes the list below and all of the above):\n",
    " * 501 = Shrub decidious    \n",
    " * 502 = Shrub evergreen    \n",
    " * 3 = Grass C3 natural\n",
    " * 4 = Grass C4 natural  \n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 102, 103, 201, 202]\n",
      "[301, 401]\n",
      "[302, 402]\n",
      "[101, 102, 103, 201, 202, 301, 401, 302, 402, 3, 4, 501, 502]\n"
     ]
    }
   ],
   "source": [
    "treecover = [101, 102, 103, 201, 202]\n",
    "cropland = [301, 401]\n",
    "pasture = [302, 402]\n",
    "vegcover = treecover + cropland + pasture + [3, 4, 501, 502]\n",
    "\n",
    "name_codes = [treecover, cropland, pasture, vegcover]\n",
    "name = ['treecover', 'cropland', 'pasture', 'vegcover']\n",
    "for x in range(0, len(name)):\n",
    "    print(name_codes[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treecover has been saved\n",
      "cropland has been saved\n",
      "pasture has been saved\n",
      "vegcover has been saved\n",
      "alpha has been saved\n",
      "lightning has been saved\n",
      "relative_humidity has been saved\n"
     ]
    }
   ],
   "source": [
    "for nam, dat in stash_conFIRE.items():\n",
    "    \n",
    "    # Set constraints and load data\n",
    "    stash_constraint = iris.AttributeConstraint(STASH = dat)\n",
    "    cubes = iris.load_cube(dir1 + file2, stash_constraint)\n",
    "    \n",
    "    \n",
    "    # Pulling out different vegetation layers for vegcover, treecover, cropland and pasture\n",
    "    if dat == 'm01s03i317':\n",
    "        \n",
    "        for var_type in range(0,len(name_codes)):\n",
    "            index = [cubes.coord('pseudo_level').points == x  for x in name_codes[var_type]]\n",
    "        \n",
    "            # This combines all the boolean arrays together. True + False = True\n",
    "            index = np.any(index, axis = 0)\n",
    "            #print('Indices for ' + name[var_type])\n",
    "            #print(index)\n",
    "        \n",
    "            # Extracts just the layers we want and saves\n",
    "            cube = cubes[index]\n",
    "            print(name[var_type] + ' has been saved')\n",
    "            out = outfile + name[var_type] + '1929oct.nc'\n",
    "            iris.save(cube, out)\n",
    "    \n",
    "    # alpha\n",
    "    elif dat == 'm01s08i223':\n",
    "        \n",
    "        # We just want the top soil layer for moisture (alpha)\n",
    "        index_soil = [cube.coord('depth').points == 0.05]\n",
    "        index_soil = np.any(index_soil, axis = 0) # Still keep this in - it makes the cube happy\n",
    "        cube_soil = cubes[index_soil]\n",
    "        \n",
    "        print(nam + ' has been saved')\n",
    "        out = outfile + nam +'1929oct.nc'\n",
    "        iris.save(cube_soil, out)\n",
    "        \n",
    "    # We save the files of all other variables (although this probably isn't right)\n",
    "    else:\n",
    "        print(nam + ' has been saved')\n",
    "        out = outfile + nam + '1929oct.nc'\n",
    "        iris.save(cubes, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Boolean array length 27 doesn't equal dimension 12",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c698bfdb4e4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Extracts just the layers we want and saves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mcube\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcube_fractional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar_type\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' has been saved'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar_type\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'2000.nc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CEH/lib/python3.7/site-packages/iris/cube.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;31m# Index with the keys, using orthogonal slicing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2146\u001b[0m         dimension_mapping, data = iris.util._slice_data_with_keys(\n\u001b[0;32m-> 2147\u001b[0;31m             cube_data, keys)\n\u001b[0m\u001b[1;32m   2148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m         \u001b[0;31m# We don't want a view of the data, so take a copy of it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CEH/lib/python3.7/site-packages/iris/util.py\u001b[0m in \u001b[0;36m_slice_data_with_keys\u001b[0;34m(data, keys)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0mdims_mapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_slices_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mthis_slice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthis_slice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0;31m# Disallow slicings where a dimension has no points, like \"[5:5]\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CEH/lib/python3.7/site-packages/dask/array/core.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m   1440\u001b[0m         )\n\u001b[1;32m   1441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mindex2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m         \u001b[0mdependencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CEH/lib/python3.7/site-packages/dask/array/slicing.py\u001b[0m in \u001b[0;36mnormalize_index\u001b[0;34m(idx, shape)\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnone_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m             \u001b[0mcheck_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnone_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CEH/lib/python3.7/site-packages/dask/array/slicing.py\u001b[0m in \u001b[0;36mcheck_index\u001b[0;34m(ind, dimension)\u001b[0m\n\u001b[1;32m    866\u001b[0m                 raise IndexError(\n\u001b[1;32m    867\u001b[0m                     \u001b[0;34m\"Boolean array length %s doesn't equal dimension %s\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m                     \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m                 )\n\u001b[1;32m    870\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Boolean array length 27 doesn't equal dimension 12"
     ]
    }
   ],
   "source": [
    "stash_constraint = iris.AttributeConstraint(STASH = stash_conFIRE['vegcover'])\n",
    "\n",
    "# Load all cubes\n",
    "aList =[]\n",
    "cube_list = iris.cube.CubeList()\n",
    "for f in files: \n",
    "    dat = iris.load_cube(dir + f, stash_constraint)\n",
    "    aList.append(dat)\n",
    "    print(str(f) + ' file loaded')\n",
    "\n",
    "# Merge all cubes together\n",
    "cube_list = iris.cube.CubeList(aList)\n",
    "cube_fractional = cube_list.merge_cube() \n",
    "    \n",
    "    \n",
    "for var_type in range(0,len(name_codes)):\n",
    "    index = [cube_fractional.coord('pseudo_level').points == x  for x in name_codes[var_type]]\n",
    "        \n",
    "    # This combines all the boolean arrays together. True + False = True\n",
    "    index = np.any(index, axis = 0)\n",
    "    #print('Indices for ' + name[var_type])\n",
    "    #print(index)\n",
    "        \n",
    "    # Extracts just the layers we want and saves\n",
    "    cube = cube_fractional[index]\n",
    "    print(name[var_type] + ' has been saved')\n",
    "    out = outfile + name[var_type] + '2000.nc'\n",
    "    iris.save(cube, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AlphaMax and alpha\n",
    "\n",
    "To create alphaMax, the maximum alpha of the previous 12 months must be divided by the mean alpha of the last 12 months and then 1 must be subtracted from the result:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\alpha_{max}}{\\alpha} -1\n",
    "\\end{equation}\n",
    "\n",
    "Loading in and extracting top layer for alpha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bc179a.p52000jan.pp has loaded\n",
      "bc179a.p52000feb.pp has loaded\n",
      "bc179a.p52000mar.pp has loaded\n",
      "bc179a.p52000apr.pp has loaded\n",
      "bc179a.p52000may.pp has loaded\n",
      "bc179a.p52000jun.pp has loaded\n",
      "bc179a.p52000jul.pp has loaded\n",
      "bc179a.p52000aug.pp has loaded\n",
      "bc179a.p52000sep.pp has loaded\n",
      "bc179a.p52000oct.pp has loaded\n",
      "bc179a.p52000nov.pp has loaded\n",
      "bc179a.p52000dec.pp has loaded\n"
     ]
    }
   ],
   "source": [
    "stash_constraint = iris.AttributeConstraint(STASH = stash_conFIRE['alpha'])\n",
    "\n",
    "# Load all cubes\n",
    "aList =[]\n",
    "cube_list = iris.cube.CubeList()\n",
    "for f in files: \n",
    "    dat = iris.load_cube(dir + f, stash_constraint)\n",
    "    aList.append(dat)\n",
    "    print(str(f) + ' has loaded')\n",
    "\n",
    "# Merge all cubes together\n",
    "cube_list = iris.cube.CubeList(aList)\n",
    "cube_alpha_new = cube_list.merge_cube() \n",
    "\n",
    "# Extract just the top soil\n",
    "index_soil = [cube_alpha_new.coord('depth').points == 0.05]\n",
    "index_soil = np.any(index_soil, axis = 0) # Still keep this in - it makes the cube happy\n",
    "cube_soil = cube_alpha_new[:, index_soil]\n",
    "cube_soil.long_name = 'alpha'\n",
    "\n",
    "# Save alpha\n",
    "out = outfile + cube_soil.long_name + '2000.nc'\n",
    "iris.save(cube_soil, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating alphaMax\n",
    "\n",
    "_Note: when running this on all historical data, the first year must be neglected in order to find alphaMax._\n",
    "\n",
    "The next section takes the first x (soon to be 12) months of the original cube (cube_soil) and collapses by the mean (cube2) and max (cube3). The alphaMax calculation is then done and the results of which are saved in the alphaMax cube. Note this will have -x timepoints to all the other variables so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we're taking the 3rd time element all the way to the last time point and put it in a new cube\n",
    "# Note: we're only taking the first 3 months, because I've currently only uploaded a year's worth of data\n",
    "cube2 = cube_soil[3:,:,:,:]\n",
    "cube3 = cube_soil[3:,:,:,:]\n",
    "alphaMax = cube_soil[3:,:,:,:]\n",
    "\n",
    "nmonths = len(cube2.coord(\"time\").points)\n",
    "\n",
    "for m in range( nmonths):\n",
    "    cube2.data[m,:,:,:] = cube_soil[m:m+3,:,:,:].collapsed([\"time\"], iris.analysis.MEAN).data\n",
    "    cube3.data[m,:,:,:] = cube_soil[m:m+3,:,:,:].collapsed([\"time\"], iris.analysis.MAX).data\n",
    "    alphaMax.data[m,:,:,:] = (cube3.data[m,:,:,:] / cube2.data[m,:,:,:]) - 1\n",
    "    #alphaMax.data[m,:,:,:] = (cube3[m,:,:,:].data / cube2[m,:,:,:].data) - 1 # This does the same thing, I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphaMax\n"
     ]
    }
   ],
   "source": [
    "alphaMax.long_name = 'alphaMax'\n",
    "print(alphaMax.long_name)\n",
    "\n",
    "out = outfile + alphaMax.long_name + '2000.nc'\n",
    "iris.save(alphaMax, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is for creating output names for when multiple years are used \n",
    "\n",
    "Just replace `+ name + 1929oct` with `date[k]` and set as a counter when saving the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "k = 0\n",
    "date =[]\n",
    "\n",
    "for i in range(1850,2015):\n",
    "    for month in months:\n",
    "        date.append(str(i) + month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Something handy commands you may still need and don't want to get rid of:\n",
    "\n",
    "#date\n",
    "#i = 0\n",
    "#for c in cl_ap5:\n",
    "#    c = cl_ap5[i]\n",
    "#    #print(c.name())\n",
    "#    list_of_stash_ap5.append(c.attributes[\"STASH\"])\n",
    "#    i += 1\n",
    "#list_of_stash == stash_conFIRE.values()\n",
    "#list_of_stash_ap5\n",
    "for x in stash_conFIRE.values():\n",
    "    print(x == 'm01s03i317')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
