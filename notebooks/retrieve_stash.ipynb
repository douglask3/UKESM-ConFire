{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stash codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Remember: use ceh conda environment\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import iris\n",
    "import iris.coord_categorisation\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set variables\n",
    "\n",
    "For now, I'm removing the variables which I can't find in either apt4 or apt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bc179a.p52000jan.pp', 'bc179a.p52000feb.pp', 'bc179a.p52000mar.pp', 'bc179a.p52000apr.pp', 'bc179a.p52000may.pp', 'bc179a.p52000jun.pp', 'bc179a.p52000aug.pp', 'bc179a.p52000sep.pp', 'bc179a.p52000oct.pp', 'bc179a.p52000nov.pp', 'bc179a.p52000dec.pp']\n",
      "bc179a.p52000jan.pp\n"
     ]
    }
   ],
   "source": [
    "# Loading in data from year 2000, ap5\n",
    "dir = '../data/UKESM/historic_1/'\n",
    "year = '2000'\n",
    "files = []\n",
    "\n",
    "for month in months:\n",
    "    files.append('bc179a.p5' + year + month +'.pp')\n",
    " \n",
    "\n",
    "print(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating alphaMax\n",
    "\n",
    "stash_constraint = iris.AttributeConstraint(STASH = 'm01s08i223')\n",
    "\n",
    "aList =[]\n",
    "cube_list = iris.cube.CubeList()\n",
    "for f in files: \n",
    "    dat = iris.load_cube(dir + f, stash_constraint)\n",
    "    aList.append(dat)\n",
    "\n",
    "cube_list = iris.cube.CubeList(aList)\n",
    "cube_alpha_new = cube_list.merge_cube() \n",
    "\n",
    "# Extract just the top soil\n",
    "index_soil = [cube_alpha_new.coord('depth').points == 0.05]\n",
    "index_soil = np.any(index_soil, axis = 0) # Still keep this in - it makes the cube happy\n",
    "cube_soil = cube_alpha_new[:, index_soil]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Unit.num2date of Unit('hours since 1970-01-01 00:00:00', calendar='360_day')>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube_soil.coord('time').units.num2date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# we need to merge the cubes of the past 12 months \n",
    "# first we need to make a cubelist\n",
    "# use cubes.merge_cube() to merge all the cubes into a single one. Note that all attributes must be the same\n",
    "# once the cubes are merge, we need to do something like this:\n",
    "\n",
    "# Note: we need to create a coord function, which will take the last 12 months and only collapse over that time.\n",
    "# To extract the previous 12 months, it might look something like this:\n",
    "def get_prev_12_months(coord, value):\n",
    "    date = coord.units.num2date(value)\n",
    "    return date.month - 11\n",
    "\n",
    "#a_mean = cube_soil.collapsed('time' , iris.analysis.MEAN)\n",
    "#a_max = cube_soil.collapsed('time' , iris.analysis.MAX)\n",
    "#alphaMax = (a_max/a_mean) - 1\n",
    "\n",
    "#print(alphaMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-11 -10]\n"
     ]
    }
   ],
   "source": [
    "#iris.coord_categorisation.add_categorised_coord(cube_soil, '12_months', 'time', get_prev_12_months)\n",
    "print(cube_soil.coord('12_months').points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '../data/'\n",
    "file = 'bc179a.p41930oct.pp'\n",
    "file2 = 'bc179a.p51929oct.pp'\n",
    "file3 = 'bc179a.p41929oct.pp'\n",
    "\n",
    "stash_conFIRE = {'vegcover'           : 'm01s03i317',\n",
    "                 'alpha'              : 'm01s08i223',\n",
    "#                'emc'                : 'm01s03i245',\n",
    "#                'treeCover'          : 'm01s03i317', # same as vegcover\n",
    "                 'lightning'          : 'm01s50i082',\n",
    "#                'pasture'            : 'm01s00i458',\n",
    "#                'population_density' : 'population_density2000-2014.nc',\n",
    "                 'relative_humidity'  : 'm01s03i245'}\n",
    "#                'cropland'           : 'm01s00i448'}\n",
    "\n",
    "# cube list\n",
    "list_of_stash_ap4 = []\n",
    "list_of_stash_ap5 = []\n",
    "\n",
    "#cl_ap4 = iris.load(dir + file3)\n",
    "#cl_ap5 = iris.load(dir + file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking to see if STASH codes match any in the file\n",
    "Uncomment last two lines in previous cell to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vegcover in ap5\n",
      "lightning in ap5\n",
      "alpha in ap5\n",
      "relative_humidity in ap5\n"
     ]
    }
   ],
   "source": [
    "#print(c.attributes['STASH'])\n",
    "\n",
    "i = 0\n",
    "for c5 in cl_ap5:\n",
    "    c5 = cl_ap5[i]\n",
    "    for name, dat in stash_conFIRE.items():\n",
    "        if c5.attributes[\"STASH\"] == dat:\n",
    "            print(name + \" in ap5\")\n",
    "    i += 1\n",
    "\n",
    "j = 0\n",
    "for c4 in cl_ap4:\n",
    "    c4 = cl_ap4[j]\n",
    "    for name, dat in stash_conFIRE.items():\n",
    "        if c4.attributes[\"STASH\"] == dat:\n",
    "            print(name + \" in ap4\")\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable location:\n",
    "\n",
    "### ap5\n",
    "\n",
    "* ``vegcover``\n",
    "* ``treecover``\n",
    "* ``alpha``\n",
    "* ``relative_humdity``\n",
    "* ``lightning``\n",
    "* `pasture`\n",
    "* `cropland`\n",
    "\n",
    "Everything appears to be in apt5, so we're going to focus on just apt5 files for now. The fractional cover (`'m01s03i317'`) consists of 27 different vegetation types. Below is a key to identify the coordinate codes (these can be viewed with `cube.coord('psuedo_level').points`. Similar for alpha, we only want the top level of moisture, so just the first layer has to be extracted.\n",
    "\n",
    "* `treecover`\n",
    "   * 101 = Broadlead deciduous tree\n",
    "   * 102 = Broadlead evergreen tree   \n",
    "   * 103  =  Broadlead  temperate evergreen tree\n",
    "   * 201 = Needleleaf deciduous    \n",
    "   * 202 = Needleleaf evergreen  \n",
    "* `cropland`\n",
    " * 301 = Grass C3 crop    \n",
    " * 401 = Grass C4 crop    \n",
    "* `pasture`\n",
    " * 302 = Grass C3 pature    \n",
    " * 402 = Grass C4 pasture\n",
    "* `vegcover` (This includes the list below and all of the above):\n",
    " * 501 = Shrub decidious    \n",
    " * 502 = Shrub evergreen    \n",
    " * 3 = Grass C3 natural\n",
    " * 4 = Grass C4 natural  \n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 102, 103, 201, 202]\n",
      "[301, 401]\n",
      "[302, 402]\n",
      "[101, 102, 103, 201, 202, 301, 401, 302, 402, 3, 4, 501, 502]\n"
     ]
    }
   ],
   "source": [
    "treecover = [101, 102, 103, 201, 202]\n",
    "cropland = [301, 401]\n",
    "pasture = [302, 402]\n",
    "vegcover = treecover + cropland + pasture + [3, 4, 501, 502]\n",
    "\n",
    "name_codes = [treecover, cropland, pasture, vegcover]\n",
    "name = ['treecover', 'cropland', 'pasture', 'vegcover']\n",
    "for x in range(0, len(name)):\n",
    "    print(name_codes[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treecover has been saved\n",
      "cropland has been saved\n",
      "pasture has been saved\n",
      "vegcover has been saved\n",
      "alpha has been saved\n",
      "lightning has been saved\n",
      "relative_humidity has been saved\n"
     ]
    }
   ],
   "source": [
    "for nam, dat in stash_conFIRE.items():\n",
    "    \n",
    "    # Set constraints and load data\n",
    "    stash_constraint = iris.AttributeConstraint(STASH = dat)\n",
    "    cubes = iris.load_cube(dir + file2, stash_constraint)\n",
    "    \n",
    "    \n",
    "    # Pulling out different vegetation layers for vegcover, treecover, cropland and pasture\n",
    "    if dat == 'm01s03i317':\n",
    "        \n",
    "        for var_type in range(0,len(name_codes)):\n",
    "            index = [cubes.coord('pseudo_level').points == x  for x in name_codes[var_type]]\n",
    "        \n",
    "            # This combines all the boolean arrays together. True + False = True\n",
    "            index = np.any(index, axis = 0)\n",
    "            #print('Indices for ' + name[var_type])\n",
    "            #print(index)\n",
    "        \n",
    "            # Extracts just the layers we want and saves\n",
    "            cube = cubes[index]\n",
    "            print(name[var_type] + ' has been saved')\n",
    "            outfile = '../outputs/stash_out/' + name[var_type] + '1929oct.nc'\n",
    "            iris.save(cube, outfile)\n",
    "    \n",
    "    # alpha\n",
    "    elif dat == 'm01s08i223':\n",
    "        \n",
    "        # We just want the top soil layer for moisture (alpha)\n",
    "        index_soil = [cube.coord('depth').points == 0.05]\n",
    "        index_soil = np.any(index_soil, axis = 0) # Still keep this in - it makes the cube happy\n",
    "        cube_soil = cubes[index_soil]\n",
    "        \n",
    "        print(nam + ' has been saved')\n",
    "        outfile = '../output/stash_out/' + nam +'1929oct.nc'\n",
    "        \n",
    "    # We save the files of all other variables (although this probably isn't right)\n",
    "    else:\n",
    "        print(nam + ' has been saved')\n",
    "        outfile = '../outputs/stash_out/' + nam + '1929oct.nc'\n",
    "        iris.save(cubes, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AlphaMax\n",
    "\n",
    "To create alphaMax, the maximum alpha of the previous 12 months must be divided by the mean alpha of the last 12 months and then 1 must be subtracted from the result:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\alpha_{max}}{\\alpha} -1\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stash_constraint = iris.AttributeConstraint(STASH = 'm01s08i223')\n",
    "\n",
    "# Load all cubes\n",
    "aList =[]\n",
    "cube_list = iris.cube.CubeList()\n",
    "for f in files: \n",
    "    dat = iris.load_cube(dir + f, stash_constraint)\n",
    "    aList.append(dat)\n",
    "\n",
    "# Merge all cubes together\n",
    "cube_list = iris.cube.CubeList(aList)\n",
    "cube_alpha_new = cube_list.merge_cube() \n",
    "\n",
    "# Extract just the top soil\n",
    "index_soil = [cube_alpha_new.coord('depth').points == 0.05]\n",
    "index_soil = np.any(index_soil, axis = 0) # Still keep this in - it makes the cube happy\n",
    "cube_soil = cube_alpha_new[:, index_soil]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to merge the cubes of the past 12 months \n",
    "# first we need to make a cubelist\n",
    "# use cubes.merge_cube() to merge all the cubes into a single one. Note that all attributes must be the same\n",
    "# once the cubes are merge, we need to do something like this:\n",
    "\n",
    "##a_mean = cube_soil.collapsed('time' , iris.analysis.MEAN)\n",
    "##a_max = cube_soil.collapsed('time' , iris.analysis.MAX)\n",
    "##alphaMax = (a_max/a_mean) - 1\n",
    "\n",
    "\n",
    "# Note: we need to create a coord function, which will take the last 12 months and only collapse over that time.\n",
    "# To extract the previous 12 months, it might look something like this:\n",
    "def get_prev_12_months(coord, value):\n",
    "    date = coord.units.num2date(value)\n",
    "    return date.month - 11\n",
    "\n",
    "\n",
    "\n",
    "#print(alphaMax)\n",
    "\n",
    "#iris.coord_categorisation.add_categorised_coord(cube_soil, '12_months', 'time', get_prev_12_months)\n",
    "print(cube_soil.coord('12_months').points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is for creating output names for when multiple years are used \n",
    "\n",
    "Just replace `+ name + 1929oct` with `date[k]` and set as a counter when saving the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul,' 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "k = 0\n",
    "date =[]\n",
    "\n",
    "for i in range(1850,2015):\n",
    "    for month in months:\n",
    "        date.append(str(i) + month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Something handy commands you may still need and don't want to get rid of:\n",
    "\n",
    "#date\n",
    "#i = 0\n",
    "#for c in cl_ap5:\n",
    "#    c = cl_ap5[i]\n",
    "#    #print(c.name())\n",
    "#    list_of_stash_ap5.append(c.attributes[\"STASH\"])\n",
    "#    i += 1\n",
    "#list_of_stash == stash_conFIRE.values()\n",
    "#list_of_stash_ap5\n",
    "for x in stash_conFIRE.values():\n",
    "    print(x == 'm01s03i317')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
