{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stash codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember: use ceh conda environment\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import iris\n",
    "import iris.coord_categorisation\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set variables\n",
    "\n",
    "For now, I'm removing the variables which I can't find in either apt4 or apt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bc179a.p52000jan.pp', 'bc179a.p52000feb.pp', 'bc179a.p52000mar.pp', 'bc179a.p52000apr.pp', 'bc179a.p52000may.pp', 'bc179a.p52000jun.pp', 'bc179a.p52000jul.pp', 'bc179a.p52000aug.pp', 'bc179a.p52000sep.pp', 'bc179a.p52000oct.pp', 'bc179a.p52000nov.pp', 'bc179a.p52000dec.pp']\n"
     ]
    }
   ],
   "source": [
    "# Loading in data from year 2000, ap5\n",
    "dir = '../data/UKESM/historic_1/'\n",
    "outfile = '../data/UKESM/retrieved_codes/'\n",
    "year = '2000'\n",
    "months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "files = []\n",
    "\n",
    "for month in months:\n",
    "    files.append('bc179a.p5' + year + month +'.pp')\n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir1 = '../data/'\n",
    "file = 'bc179a.p41930oct.pp'\n",
    "file2 = 'bc179a.p51929oct.pp'\n",
    "file3 = 'bc179a.p41929oct.pp'\n",
    "\n",
    "stash_conFIRE = {'vegcover'           : 'm01s03i317',\n",
    "                 'alpha'              : 'm01s08i223',\n",
    "#                'emc'                : 'm01s03i245',\n",
    "#                'treeCover'          : 'm01s03i317', # same as vegcover\n",
    "                 'lightning'          : 'm01s50i082',\n",
    "#                'pasture'            : 'm01s00i458',\n",
    "#                'population_density' : 'population_density2000-2014.nc',\n",
    "                 'relative_humidity'  : 'm01s03i245'}\n",
    "#                'cropland'           : 'm01s00i448'}\n",
    "\n",
    "# cube list\n",
    "list_of_stash_ap4 = []\n",
    "list_of_stash_ap5 = []\n",
    "\n",
    "#cl_ap4 = iris.load(dir1 + file3)\n",
    "#cl_ap5 = iris.load(dir1 + file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking to see if STASH codes match any in the file\n",
    "Uncomment last two lines in previous cell to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vegcover in ap5\n",
      "lightning in ap5\n",
      "alpha in ap5\n",
      "relative_humidity in ap5\n"
     ]
    }
   ],
   "source": [
    "#print(c.attributes['STASH'])\n",
    "\n",
    "i = 0\n",
    "for c5 in cl_ap5:\n",
    "    c5 = cl_ap5[i]\n",
    "    for name, dat in stash_conFIRE.items():\n",
    "        if c5.attributes[\"STASH\"] == dat:\n",
    "            print(name + \" in ap5\")\n",
    "    i += 1\n",
    "\n",
    "j = 0\n",
    "for c4 in cl_ap4:\n",
    "    c4 = cl_ap4[j]\n",
    "    for name, dat in stash_conFIRE.items():\n",
    "        if c4.attributes[\"STASH\"] == dat:\n",
    "            print(name + \" in ap4\")\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable location:\n",
    "\n",
    "### ap5\n",
    "\n",
    "* ``vegcover``\n",
    "* ``treecover``\n",
    "* ``alpha``\n",
    "* ``relative_humdity``\n",
    "* ``lightning``\n",
    "* `pasture`\n",
    "* `cropland`\n",
    "\n",
    "Everything appears to be in apt5, so we're going to focus on just apt5 files for now. The fractional cover (`'m01s03i317'`) consists of 27 different vegetation types. Below is a key to identify the coordinate codes (these can be viewed with `cube.coord('psuedo_level').points`. Similar for alpha, we only want the top level of moisture, so just the first layer has to be extracted.\n",
    "\n",
    "* `treecover`\n",
    "   * 101 = Broadlead deciduous tree\n",
    "   * 102 = Broadlead evergreen tree   \n",
    "   * 103  =  Broadlead  temperate evergreen tree\n",
    "   * 201 = Needleleaf deciduous    \n",
    "   * 202 = Needleleaf evergreen  \n",
    "* `cropland`\n",
    " * 301 = Grass C3 crop    \n",
    " * 401 = Grass C4 crop    \n",
    "* `pasture`\n",
    " * 302 = Grass C3 pature    \n",
    " * 402 = Grass C4 pasture\n",
    "* `vegcover` (This includes the list below and all of the above):\n",
    " * 501 = Shrub decidious    \n",
    " * 502 = Shrub evergreen    \n",
    " * 3 = Grass C3 natural\n",
    " * 4 = Grass C4 natural  \n",
    "   \n",
    "_Note: You'll need to alter all the output cubes to skip the first year of data (in order to make them the same length as alphaMax). You'll also need to change `m + 3` to `m + 11` so it iterates over 12 months, instead of 3. Lines in the script that need this changing will be labelled #xxx._\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 102, 103, 201, 202]\n",
      "[301, 401]\n",
      "[302, 402]\n",
      "[101, 102, 103, 201, 202, 301, 401, 302, 402, 3, 4, 501, 502]\n"
     ]
    }
   ],
   "source": [
    "treecover = [101, 102, 103, 201, 202]\n",
    "cropland = [301, 401]\n",
    "pasture = [302, 402]\n",
    "vegcover = treecover + cropland + pasture + [3, 4, 501, 502]\n",
    "\n",
    "name_codes = [treecover, cropland, pasture, vegcover]\n",
    "name = ['treecover', 'cropland', 'pasture', 'vegcover']\n",
    "for x in range(0, len(name)):\n",
    "    print(name_codes[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treecover has been saved\n",
      "cropland has been saved\n",
      "pasture has been saved\n",
      "vegcover has been saved\n",
      "alpha has been saved\n",
      "lightning has been saved\n",
      "relative_humidity has been saved\n"
     ]
    }
   ],
   "source": [
    "# for nam, dat in stash_conFIRE.items():\n",
    "    \n",
    "#     # Set constraints and load data\n",
    "#     stash_constraint = iris.AttributeConstraint(STASH = dat)\n",
    "#     cubes = iris.load_cube(dir1 + file2, stash_constraint)\n",
    "    \n",
    "    \n",
    "#     # Pulling out different vegetation layers for vegcover, treecover, cropland and pasture\n",
    "#     if dat == 'm01s03i317':\n",
    "        \n",
    "#         for var_type in range(0,len(name_codes)):\n",
    "#             index = [cubes.coord('pseudo_level').points == x  for x in name_codes[var_type]]\n",
    "        \n",
    "#             # This combines all the boolean arrays together. True + False = True\n",
    "#             index = np.any(index, axis = 0)\n",
    "#             #print('Indices for ' + name[var_type])\n",
    "#             #print(index)\n",
    "        \n",
    "#             # Extracts just the layers we want and saves\n",
    "#             cube = cubes[index]\n",
    "#             print(name[var_type] + ' has been saved')\n",
    "#             out = outfile + name[var_type] + '1929oct.nc'\n",
    "#             iris.save(cube, out)\n",
    "    \n",
    "#     # alpha\n",
    "#     elif dat == 'm01s08i223':\n",
    "        \n",
    "#         # We just want the top soil layer for moisture (alpha)\n",
    "#         index_soil = [cube.coord('depth').points == 0.05]\n",
    "#         index_soil = np.any(index_soil, axis = 0) # Still keep this in - it makes the cube happy\n",
    "#         cube_soil = cubes[index_soil]\n",
    "        \n",
    "#         print(nam + ' has been saved')\n",
    "#         out = outfile + nam +'1929oct.nc'\n",
    "#         iris.save(cube_soil, out)\n",
    "        \n",
    "#     # We save the files of all other variables (although this probably isn't right)\n",
    "#     else:\n",
    "#         print(nam + ' has been saved')\n",
    "#         out = outfile + nam + '1929oct.nc'\n",
    "#         iris.save(cubes, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving relative_humidity and lightning\n",
    "\n",
    "The stash code for both relative_humidity lightning only have 3 dimensions. Because all the other variables have 4 dimensions, it would be easiest for the next stage (which is turning the data into a .csv) if we added an extra dimension (?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bc179a.p52000jan.pp file loaded\n",
      "bc179a.p52000feb.pp file loaded\n",
      "bc179a.p52000mar.pp file loaded\n",
      "bc179a.p52000apr.pp file loaded\n",
      "bc179a.p52000may.pp file loaded\n",
      "bc179a.p52000jun.pp file loaded\n",
      "bc179a.p52000jul.pp file loaded\n",
      "bc179a.p52000aug.pp file loaded\n",
      "bc179a.p52000sep.pp file loaded\n",
      "bc179a.p52000oct.pp file loaded\n",
      "bc179a.p52000nov.pp file loaded\n",
      "bc179a.p52000dec.pp file loaded\n",
      "lightning has been saved\n",
      "bc179a.p52000jan.pp file loaded\n",
      "bc179a.p52000feb.pp file loaded\n",
      "bc179a.p52000mar.pp file loaded\n",
      "bc179a.p52000apr.pp file loaded\n",
      "bc179a.p52000may.pp file loaded\n",
      "bc179a.p52000jun.pp file loaded\n",
      "bc179a.p52000jul.pp file loaded\n",
      "bc179a.p52000aug.pp file loaded\n",
      "bc179a.p52000sep.pp file loaded\n",
      "bc179a.p52000oct.pp file loaded\n",
      "bc179a.p52000nov.pp file loaded\n",
      "bc179a.p52000dec.pp file loaded\n",
      "relative_humidity has been saved\n"
     ]
    }
   ],
   "source": [
    "stash_l = [ 'lightning', 'relative_humidity']\n",
    "\n",
    "for l in stash_l:\n",
    "    stash_constraint = iris.AttributeConstraint(STASH = stash_conFIRE[l])\n",
    "\n",
    "    # Load all cubes\n",
    "    aList =[]\n",
    "    cube_list = iris.cube.CubeList()\n",
    "    for f in files: \n",
    "        dat = iris.load_cube(dir + f, stash_constraint)\n",
    "        aList.append(dat)\n",
    "        print(str(f) + ' file loaded')\n",
    "\n",
    "    # Merge all cubes together\n",
    "    cube_list = iris.cube.CubeList(aList)\n",
    "    cubes = cube_list.merge_cube()\n",
    "\n",
    "    # For skipping the first x months\n",
    "    #xxx\n",
    "    cubes = cubes[3:,:,:]\n",
    "\n",
    "    print(l + ' has been saved')\n",
    "    out = outfile + l + '2000.nc'\n",
    "    iris.save(cubes, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving variables from fractional cover "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bc179a.p52000jan.pp file loaded\n",
      "bc179a.p52000feb.pp file loaded\n",
      "bc179a.p52000mar.pp file loaded\n",
      "bc179a.p52000apr.pp file loaded\n",
      "bc179a.p52000may.pp file loaded\n",
      "bc179a.p52000jun.pp file loaded\n",
      "bc179a.p52000jul.pp file loaded\n",
      "bc179a.p52000aug.pp file loaded\n",
      "bc179a.p52000sep.pp file loaded\n",
      "bc179a.p52000oct.pp file loaded\n",
      "bc179a.p52000nov.pp file loaded\n",
      "bc179a.p52000dec.pp file loaded\n",
      "Indices for treecover\n",
      "treecover has been saved\n",
      "Indices for cropland\n",
      "cropland has been saved\n",
      "Indices for pasture\n",
      "pasture has been saved\n",
      "Indices for vegcover\n",
      "vegcover has been saved\n"
     ]
    }
   ],
   "source": [
    "stash_constraint = iris.AttributeConstraint(STASH = stash_conFIRE['vegcover'])\n",
    "\n",
    "# Load all cubes\n",
    "aList =[]\n",
    "cube_list = iris.cube.CubeList()\n",
    "for f in files: \n",
    "    dat = iris.load_cube(dir + f, stash_constraint)\n",
    "    aList.append(dat)\n",
    "    print(str(f) + ' file loaded')\n",
    "\n",
    "# Merge all cubes together\n",
    "cube_list = iris.cube.CubeList(aList)\n",
    "cube_fractional = cube_list.merge_cube() \n",
    "    \n",
    "    \n",
    "for var_type in range(0,len(name_codes)):\n",
    "    index = [cube_fractional.coord('pseudo_level').points == x  for x in name_codes[var_type]]\n",
    "        \n",
    "    # This combines all the boolean arrays together. True + False = True\n",
    "    index = np.any(index, axis = 0)\n",
    "    print('Indices for ' + name[var_type])\n",
    "    #print(index)\n",
    "        \n",
    "    # Extracts just the layers we want and saves\n",
    "    cube = cube_fractional[:,index]\n",
    "    \n",
    "    # For skipping the first x months\n",
    "    #xxx\n",
    "    cube = cube[3:,:,:,:].collapsed(['pseudo_level'], iris.analysis.SUM)\n",
    "    \n",
    "    out = outfile + name[var_type] + '2000.nc'\n",
    "    iris.save(cube, out)\n",
    "    print(name[var_type] + ' has been saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AlphaMax and alpha\n",
    "\n",
    "To create alphaMax, the maximum alpha of the previous 12 months must be divided by the mean alpha of the last 12 months and then 1 must be subtracted from the result:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\alpha_{max}}{\\alpha} -1\n",
    "\\end{equation}\n",
    "\n",
    "Loading in and extracting top layer for alpha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bc179a.p52000jan.pp has loaded\n",
      "bc179a.p52000feb.pp has loaded\n",
      "bc179a.p52000mar.pp has loaded\n",
      "bc179a.p52000apr.pp has loaded\n",
      "bc179a.p52000may.pp has loaded\n",
      "bc179a.p52000jun.pp has loaded\n",
      "bc179a.p52000jul.pp has loaded\n",
      "bc179a.p52000aug.pp has loaded\n",
      "bc179a.p52000sep.pp has loaded\n",
      "bc179a.p52000oct.pp has loaded\n",
      "bc179a.p52000nov.pp has loaded\n",
      "bc179a.p52000dec.pp has loaded\n"
     ]
    }
   ],
   "source": [
    "stash_constraint = iris.AttributeConstraint(STASH = stash_conFIRE['alpha'])\n",
    "\n",
    "# Load all cubes\n",
    "aList =[]\n",
    "cube_list = iris.cube.CubeList()\n",
    "for f in files: \n",
    "    dat = iris.load_cube(dir + f, stash_constraint)\n",
    "    aList.append(dat)\n",
    "    print(str(f) + ' has loaded')\n",
    "\n",
    "# Merge all cubes together\n",
    "cube_list = iris.cube.CubeList(aList)\n",
    "cube_alpha_new = cube_list.merge_cube() \n",
    "\n",
    "# Extract just the top soil\n",
    "index_soil = [cube_alpha_new.coord('depth').points == 0.05]\n",
    "index_soil = np.any(index_soil, axis = 0) # Still keep this in - it makes the cube happy\n",
    "cube_soil = cube_alpha_new[:, index_soil]\n",
    "cube_soil.long_name = 'alpha'\n",
    "\n",
    "#xxx\n",
    "cube_soil_skip_year = cube_soil[3:,0,:,:]\n",
    "\n",
    "\n",
    "# Save alpha\n",
    "out = outfile + cube_soil.long_name + '2000.nc'\n",
    "iris.save(cube_soil_skip_year, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating alphaMax\n",
    "\n",
    "_Note: when running this on all historical data, the first year must be neglected in order to find alphaMax._\n",
    "\n",
    "The next section takes the first x (soon to be 12) months of the original cube (cube_soil) and collapses by the mean (cube2) and max (cube3). The alphaMax calculation is then done and the results of which are saved in the alphaMax cube. Note this will have -x timepoints to all the other variables so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we're taking the 3rd time element all the way to the last time point and put it in a new cube\n",
    "# Note: we're only taking the first 3 months, because I've currently only uploaded a year's worth of data\n",
    "\n",
    "#xxx\n",
    "cube2 = cube_soil[3:,:,:,:]\n",
    "cube3 = cube_soil[3:,:,:,:]\n",
    "alphaMax = cube_soil[3:,:,:,:]\n",
    "\n",
    "nmonths = len(cube2.coord(\"time\").points)\n",
    "\n",
    "#xxx\n",
    "for m in range( nmonths):\n",
    "    cube2.data[m,:,:,:] = cube_soil[m:m+3,:,:,:].collapsed([\"time\"], iris.analysis.MEAN).data\n",
    "    cube3.data[m,:,:,:] = cube_soil[m:m+3,:,:,:].collapsed([\"time\"], iris.analysis.MAX).data\n",
    "    alphaMax.data[m,:,:,:] = (cube3.data[m,0,:,:] / cube2.data[m,0,:,:]) - 1\n",
    "    #alphaMax.data[m,:,:,:] = (cube3[m,:,:,:].data / cube2[m,:,:,:].data) - 1 # This does the same thing, I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphaMax\n"
     ]
    }
   ],
   "source": [
    "alphaMax.long_name = 'alphaMax'\n",
    "print(alphaMax.long_name)\n",
    "alphaMax  = alphaMax.collapsed(['depth'], iris.analysis.SUM)\n",
    "\n",
    "out = outfile + alphaMax.long_name + '2000.nc'\n",
    "iris.save(alphaMax, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is for creating output names for when multiple years are used \n",
    "\n",
    "Just replace `+ name + 1929oct` with `date[k]` and set as a counter when saving the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "k = 0\n",
    "date =[]\n",
    "\n",
    "for i in range(1850,2015):\n",
    "    for month in months:\n",
    "        date.append(str(i) + month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Something handy commands you may still need and don't want to get rid of:\n",
    "\n",
    "#date\n",
    "#i = 0\n",
    "#for c in cl_ap5:\n",
    "#    c = cl_ap5[i]\n",
    "#    #print(c.name())\n",
    "#    list_of_stash_ap5.append(c.attributes[\"STASH\"])\n",
    "#    i += 1\n",
    "#list_of_stash == stash_conFIRE.values()\n",
    "#list_of_stash_ap5\n",
    "for x in stash_conFIRE.values():\n",
    "    print(x == 'm01s03i317')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m01s03i317 / (unknown)              (time: 12; pseudo_level: 27; latitude: 144; longitude: 192)\n",
      "     Dimension coordinates:\n",
      "          time                           x                 -             -               -\n",
      "          pseudo_level                   -                 x             -               -\n",
      "          latitude                       -                 -             x               -\n",
      "          longitude                      -                 -             -               x\n",
      "     Auxiliary coordinates:\n",
      "          forecast_period                x                 -             -               -\n",
      "     Scalar coordinates:\n",
      "          forecast_reference_time: 1850-01-01 00:00:00\n",
      "     Attributes:\n",
      "          STASH: m01s03i317\n",
      "          source: Data from Met Office Unified Model\n",
      "          um_version: 10.9\n",
      "     Cell methods:\n",
      "          mean: time (1 hour)\n"
     ]
    }
   ],
   "source": [
    "print(cube_fractional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be one file for each of the drivers. These should be fed into the prepare_data script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
