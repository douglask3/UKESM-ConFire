{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<table style=\"width: 100%; border-collapse: collapse;\" border=\"0\">\n",
    "<tr>\n",
    "<td><b>Created:</b> Tuesday 31 January 2017</td>\n",
    "<td style=\"text-align: right;\"><a href=\"https://www.github.com/rhyswhitley/fire_limitation\">github.com/rhyswhitley/fire_limitation</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<div>\n",
    "<center>\n",
    "<font face=\"Times\">\n",
    "<br>\n",
    "<h1>Quantifying the uncertainity of a global fire limitation model using Bayesian inference</h1>\n",
    "<h2>Part 2: Bayesian inference</h2>\n",
    "<br>\n",
    "<br>\n",
    "<sup>1,* </sup>Douglas Kelley, \n",
    "<sup>2 </sup>Ioannis Bistinas,\n",
    "<sup>3 </sup>Rhys Whitley,\n",
    "<sup>4 </sup>Chantelle Burton, \n",
    "<sup>1 </sup>Tobias Marthews, \n",
    "<sup>6, 7 </sup>Ning Dong\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<sup>1 </sup>Centre for Ecology and Hydrology, Maclean Building, Crowmarsh Gifford, Wallingford, Oxfordshire, United Kingdom\n",
    "<br>\n",
    "<sup>2 </sup>Vrije Universiteit Amsterdam, Faculty of Earth and Life Sciences, Amsterdam, Netherlands\n",
    "<br>\n",
    "<sup>3 </sup>Natural Perils Pricing, Commercial & Consumer Portfolio & Pricing, Suncorp Group, Sydney, Australia\n",
    "<br>\n",
    "<sup>4 </sup>Met Office United Kingdom, Exeter, United Kingdom\n",
    "<br>\n",
    "<sup>5 </sup>Centre for Past Climate Change and School of Archaeology, Geography and Environmental Sciences (SAGES), University of Reading, Reading, United Kingdom \n",
    "<br>\n",
    "<sup>6 </sup>Department of Biological Sciences, Macquarie University, North Ryde, NSW 2109, Australia \n",
    "<br>\n",
    "<br>\n",
    "<h3>Summary</h3>\n",
    "<hr>\n",
    "<p> \n",
    "This notebook aims to quantify the model parameters of a global fire model (defined below). The model is driven by a number of covariates (X<sub>i=1, 2, ... M</sub>) that describe: cropland, pasture and urban area footprints; frequency of lightening ignitions, population density, net primary productivity (NPP) and <i>Alpha</i>, a proxy measure of available soil moisture in the root zone. The model attempts to predict the impact of fire through burnt area and is thus the model target (Y).\n",
    "</p>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<i>Python code and calculations below</i>\n",
    "<br>\n",
    "</font>\n",
    "</center>\n",
    "<hr>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model description\n",
    "The model considers percentage of burnt area to be the joint product of a set of conditions that modulate fire through fuel load, ignitions, moisture and supression. Each control assumes some equilibrium point that desribes the optimal conditions for fire, that may be proportionally modified through some empirical relationship. These are briefly outlined below for the sake of comprehension in this notebook, but can be referred to in more detail in the model protocol located in the docs/ folder (<a href='file:///localhost/../docs/Model_description.pdf'>model protocol</a>).\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    F_{burn} &=& \\prod_{i}S(x_{i}) \\\\[1em]\n",
    "\\end{eqnarray}\n",
    "\n",
    "Where $S(x_{i})$ representes some measure of fire conditions by $i =$ fuel, moisture, ignitions and anthropagenic supression, and is describe by a sigmoid:\n",
    "\n",
    "\\begin{equation}\n",
    "    S(x_{i=fuel, moist, ignite, suppr}) = \\frac{1}{1 + \\exp\\{-b\\cdot(x_i-a)\\}}\n",
    "\\end{equation}\n",
    "\n",
    "The $fuel$ sigmoid considers only fractional vegetation cover and therefore have no hyper-parameters. Sigmoids $moist$, $ignite$ and $suppr$ describe an aggregation of other climate and land-use covariates. Because these sigmoids are influenced by an aggregation of different drivers, they are influenced in turn by different sets of hyper-parameters; these are now described below.  \n",
    "\n",
    "#### Fuel load covariate (no hyper-parameters)\n",
    "\\begin{equation}\n",
    "    x_{fuel} = FPC^{p} \\cdot (v_{fuel} \\cdot (\\alpha_{max}/\\alpha -1) + 1)/(1+v_{fuel}) \n",
    "\\end{equation}\n",
    "\n",
    "#### Moisture covariate\n",
    "\\begin{equation}\n",
    "    x_{moist} = \\alpha + v_M \\cdot EMC    \n",
    "\\end{equation}\n",
    "\n",
    "where $EMC$ is the equilibrium moisture content.\n",
    "#### Ignition covariate \n",
    "\\begin{equation}\n",
    "    x_{ignite} = Lightn + v_p\\cdot A_{pasture} + v_{d1}\\cdot\\rho_{population}\n",
    "\\end{equation}\n",
    "\n",
    "Where $Lightn$ is the number of cloud-to-ground lightning strikes, modified  as per Kelley et al. 2014.\n",
    "\n",
    "#### Supression covariate \n",
    "\\begin{equation}\n",
    "    x_{supress} = A_{urban} + v_C\\cdot A_{Crop} + v_{d2}\\cdot\\rho_{population} \n",
    "\\end{equation}\n",
    "\n",
    "This leaves 17 free parameters that need to be optimised against observations of burnt area.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from   io     import StringIO\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import pymc3  as pm3 \n",
    "from   pymc3.backends import SQLite\n",
    "from   scipy  import optimize\n",
    "from   theano import tensor as tt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# setup nice plotting\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# paths and parameters\n",
    "                    # Full model\n",
    "sample_pc     = 2 # = 10\n",
    "nIterations   = 5000 # = 10000\n",
    "nJobs         = 2   # = 2\n",
    "#chains per job\n",
    "nChains       = 2   # = 3\n",
    "#how many times will you smple the postirior\n",
    "n_posterior_sample = 100 # = 100 for analysis = 1000 for publication.\n",
    "\n",
    "\n",
    "datPath       = \"../data/globfire.csv\"\n",
    "param_outpath = '../outputs/params-test.csv'\n",
    "trace_db      = '../data/firemodel_trace.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npLog0 (x):\n",
    "    return [np.log(i) if i > 0 else 0.0000001 for i in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.1 Fire limitation model definition\n",
    "\n",
    "Could possibly contain this in a class object, but I'm not sure theano can instantiate the object to be used by the GPU. If I've made absolutely no sense just then, then I would leave the following as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb import set_trace as browser\n",
    "def fuel_load(fuel, alphaMax, fp, fpg):\n",
    "    \"\"\"\n",
    "    Definition to describe fuel load: while return the input; capability to be modified later.\n",
    "    \"\"\"\n",
    "  \n",
    "    return (fuel**fp) * (fpg * alphaMax + 1) / (1 + fpg)\n",
    "\n",
    "def moisture(alpha, emc, tree, cM, cMT):\n",
    "    \"\"\"\n",
    "    Definition to describe moisture\n",
    "    \"\"\"\n",
    "    return (alpha + 0.01*cM*emc + 0.01*cMT * tree) / (1 + cM + cMT)\n",
    "\n",
    "\n",
    "def ignition(lightning, pasture_area, pop_density, cP, cD1):\n",
    "    \"\"\"\n",
    "    Definition for the measure of ignition\n",
    "    \"\"\"\n",
    "    ignite = lightning + cP*pasture_area + cD1*pop_density\n",
    "    #try:\n",
    "    #    ignite = npLog0(ignite)\n",
    "    #except:\n",
    "    #    ignite = np.log(ignite)\n",
    "    return ignite\n",
    "\n",
    "def supression(crop_area, pop_density, cD2):\n",
    "    \"\"\"\n",
    "    Definition for the measure of fire supression\n",
    "    \"\"\"\n",
    "    return crop_area + cD2*pop_density\n",
    "\n",
    "def tt_sigmoid(x, k, x0):\n",
    "    \"\"\"\n",
    "    Sigmoid function to describe limitation using tensor\n",
    "    \"\"\"\n",
    "    return 1.0/(1.0 + tt.exp(-k*(x - x0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Import data\n",
    "\n",
    "Load data and do any necessary transformation needed for the Bayesian modelling framework. Rows are defined as a fraction of total data points (above). For full optimiatizaion, we set at 10%, but for testing purposes I've limited the number of rows I'm importing to 2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_with_buffer(filename, line_select, **kwargs):\n",
    "    s_buf = StringIO()\n",
    "    line_select = np.sort(line_select)\n",
    "    with open(filename) as file:\n",
    "        count = -1\n",
    "        lineN = -1\n",
    "        for line in file:\n",
    "            lineN += 1\n",
    "            if lineN == 0 or lineN == line_select[count]:\n",
    "                s_buf.write(line)\n",
    "                count += 1\n",
    "                if count == len(line_select): break\n",
    "            \n",
    "    s_buf.seek(0)\n",
    "    df = pd.read_csv(s_buf,**kwargs)\n",
    "    return df\n",
    "\n",
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f): pass\n",
    "    return i + 1\n",
    "\n",
    "DATAPATH = os.path.expanduser(datPath)\n",
    "\n",
    "nlines      = file_len(DATAPATH)\n",
    "npoints     = round(sample_pc * nlines / 100)\n",
    "line_select = np.random.choice(range(0, nlines), npoints, False)\n",
    "fd          = load_with_buffer(DATAPATH, line_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd['fuel'] = npLog0(fd['NPP'].values)\n",
    "fd['vegCover'] = (100.0 - fd['bareground'])/100.0\n",
    "fd['alphaMax'] = fd['alpha_'] - 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a sanity check to make sure our data has imported correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fd.info()\n",
    "fd.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Baysian framework\n",
    "\n",
    "A simple explanation of Baye's law is:\n",
    "\n",
    "\\begin{equation}\n",
    "    P(\\beta|X) \\propto P(\\beta)\\cdot P(X|\\beta)\n",
    "\\end{equation}\n",
    "\n",
    "where $X$ is our data (observations of some arbitrary system), and $\\beta$ our set of unexplained parameters that describe the reponse of our _proposed understanding_ of this system as it varies with $X$.\n",
    "\n",
    "### 2.3.1 Prior definitions\n",
    "Because I have no idea what the uncertainty on the hyper parameters should look like (beyond $\\beta> 0$), I've set them all as uniform, where bounds are either physical limits of the variable in question, or set generously beyound what is realistic plausable. Some of them can possibly be describe as exponential or half-normal, due to the physical nature of $\\beta$, but we can play around with that later.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    P(\\beta) &=& \\prod_{i=1}^{4}P(a_i)\\prod_{i=1}^{4}P(b_i)\\cdot P(\\sigma)\\cdot P(v_c)P(v_p)P(v_{d,1})P(v_{d,2}) \\\\[1.5em]\n",
    "    P(a) = P(b) = P(\\sigma) &=& \\mathcal{N}(0, 1) \\\\[1em]\n",
    "    P(v_c) = P(v_p) = P(v_{d,1}) = P(v_{d,2}) &=& \\mathcal{U}(\\beta_{\\min}, \\beta_{\\max}) \\\\[1.5em]\n",
    "\\end{eqnarray}\n",
    "\n",
    "I'm not totally sure about the maths above being right, but it's just to show that _full_ prior is normal. Important, because we'll also describe the error (likelihood) as normal, such that the posterior is therefore normal (conjugate); i.e. $\\mathcal{N}\\times\\mathcal{N}=\\mathcal{N}$ (expansion happens in the mean of the exponent). \n",
    "\n",
    "Back to the code.., `pymc3` is quite funky in that it allows me to create an empty `Model()` object and just add things to it as I need them using a `with` statement. I've called our Bayesian model `fire_error` as that is what we are trying to Quantify.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm3.Model() as fire_error:\n",
    "    \n",
    "# first for the sigmoids  \n",
    "    fuel_x0        = pm3.Uniform('fuel_x0'       ,    -1.0,  2.0)\n",
    "    fuel_k         = pm3.Uniform('fuel_k'        ,     0.0, 1000.0)\n",
    "    fuel_pw        = pm3.Uniform('fuel_pw'       ,     1.0, 20.0)\n",
    "    fuel_pg        = pm3.Uniform('fuel_pg'       ,     0.0, 1.0)\n",
    "    \n",
    "    moisture_x0    = pm3.Uniform('moisture_x0'   ,    -1.0,  2.0)\n",
    "    moisture_k     = pm3.Uniform('moisture_k'    ,     0.0, 1000.0)\n",
    "    \n",
    "    ignition_x0    = pm3.Uniform('igntions_x0'   ,    -4.0, 4.0)\n",
    "    ignition_k     = pm3.Uniform('igntions_k'    ,     0.0, 1000.0)\n",
    "    \n",
    "    suppression_x0 = pm3.Uniform('suppression_x0',  -100.0,  200.0)\n",
    "    suppression_k  = pm3.Uniform('suppression_k' ,     0.0, 10.0)\n",
    "    \n",
    "    max_f          = pm3.Uniform('max_f'         ,     0.0,    1.0)\n",
    "    #pow_f          = pm3.Uniform('pow_f'         ,     0.0,    1.0)\n",
    "# now for the hyper-parameters that describe the independent fire condition covariates\n",
    "    cM  = pm3.Uniform('cM' , 0.0, 1e4)\n",
    "    cMT = pm3.Uniform('cMT', 0.0, 1e4)\n",
    "    #cL  = pm3.Uniform('cL' , 0, 1e4)\n",
    "    cP  = pm3.Uniform('cP' , 0.0, 1e4)\n",
    "    cD1 = pm3.Uniform('cD1', 0.0, 1e4)\n",
    "    cD2 = pm3.Uniform('cD2', 0.0, 1e4)\n",
    "    #cDmax = pm3.Uniform('cDmax', 0, 1e4)\n",
    "# describe the standard deviation in the error term\n",
    "    sigma = pm3.HalfNormal('sigma', sd=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Likelihood definition\n",
    "\n",
    "For the sake of simplicity (and because I don't really know any better), we define the model error as normally distributed (i.i.d.) although it most likely isn't. We could make this more complicated later by defining the error as heteroscedastic, but I wouldn't bother with that until we have some idea of the convergence. We're describing the error (observations minus model predictions) as follows:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    P(X|\\beta) &=& \\mathcal{N}(F_{burn}, \\sigma) \\\\[1em]\n",
    "    \\mathcal{N}(F_{burn}, \\sigma) &=& \\frac{N}{\\sigma\\sqrt{2\\pi}}\\exp\\left\\{\\sum_{i=1}^{N}\\left(\\frac{y_i - F_{burn, i}}{\\sigma_i}\\right)^2\\right\\}\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $y_i$ is a set of observations we're attempting to optimise on. Below is the code that describes the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fire_error:\n",
    "    \n",
    "    # transform hyper-covariates \n",
    "    f_fuel        = fuel_load(fd[\"vegCover\"].values, fd[\"alphaMax\"].values, fuel_pw, fuel_pg)\n",
    "    \n",
    "    f_moisture    = moisture(fd[\"alpha\"].values, fd[\"emc\"].values, fd[\"treecover\"].values, cM, cMT)\n",
    "    \n",
    "    f_ignition    = ignition(fd[\"lightning_ignitions\"].values, \\\n",
    "                             fd[\"pasture\"].values, \\\n",
    "                             fd[\"population_density\"].values, \\\n",
    "                             cP, cD1)\n",
    "    \n",
    "    f_suppression = supression(fd[\"cropland\"].values, \\\n",
    "                               fd[\"population_density\"].values, \\\n",
    "                               cD2)\n",
    "    \n",
    "    # burnt area is assumed to be the product of the 4 sigmoids\n",
    "    \n",
    "    prediction = max_f * np.product([tt_sigmoid(f_fuel, fuel_k, fuel_x0),\n",
    "                             tt_sigmoid(f_moisture, - moisture_k, moisture_x0),\n",
    "                             tt_sigmoid(f_ignition, ignition_k, ignition_x0),\n",
    "                             tt_sigmoid(f_suppression, - suppression_k, suppression_x0)])\n",
    "                  \n",
    "    # calculate the error between observed and predicted burnt area\n",
    "    error = pm3.Normal('error', mu=prediction, sd=sigma, observed=fd['fire'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Posterior sampling\n",
    "\n",
    "Because it is nigh impossible to determine the posterior solution analytically we will instead sample the information space to **infer** the posterior solutions for each of the model parameters. In this case we are using a Metropolis-Hasting step MCMC.\n",
    "\n",
    "I've tried using No-U-Turn (NUTS) sampling (which is the new kid on the block), but there are issues with it's current implementation in pymc3 (see github repo issues). Can use it once problems are ironed out - but TBH it doesn't matter if we're getting a reasonable convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nChains = nChains* nJobs\n",
    "with fire_error:\n",
    "    \n",
    "    # help the sampling out by quickly finding an optimal start position\n",
    "    start = pm3.find_MAP(model=fire_error.model, fmin=optimize.fmin_powell)\n",
    "    \n",
    "    # set the step-method (criteria algorithm for moving around information space)\n",
    "    step = pm3.Metropolis()\n",
    "    \n",
    "    # save our sampling to disk so we can access it later\n",
    "    #db_save = SQLite(trace_db)\n",
    "    \n",
    "    # do the sampling\n",
    "    mcmc_traces = pm3.sample(nIterations * nChains * nJobs, step=step, start=start, njobs= nJobs, chains = nChains) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pm3.traceplot(mcmc_traces);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output paramaters\n",
    "The iterations at the start are just letting the optimization settle. So we will only sample to last 50% of iterations for futher analysis. We also expor these to csv, which others can use to do their own analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames = mcmc_traces.varnames\n",
    "\n",
    "\n",
    "def cutLastX(varname, mcmc_traces, ncut = 50):\n",
    "    vals = mcmc_traces.get_values(varname)\n",
    "    def subcut(vals, r, ncut = 50):\n",
    "        cut_np = (r+1) * round(len(vals)/nChains)\n",
    "        ncut = round(len(vals) * ncut / (nChains *100))\n",
    "        return vals[(cut_np - ncut):cut_np]\n",
    "    vals = [subcut(vals, r) for r in range(nChains)]\n",
    "    return np.array(vals).flatten()\n",
    "\n",
    "vals = [cutLastX(i, mcmc_traces) for i in varnames]\n",
    "vals = pd.DataFrame(np.array(vals).T, columns=varnames)\n",
    "\n",
    "vals.to_csv(param_outpath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let look at the pdf of the last 50% of iterations for each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "variables2Plot = ['fuel_x0'       , 'fuel_k',\n",
    "                  'moisture_x0'   , 'moisture_k',\n",
    "                  'igntions_x0'   , 'igntions_k',\n",
    "                  'suppression_x0', 'suppression_k',\n",
    "                  'fuel_pw'       , 'fuel_pg',\n",
    "                  'cM'            , 'cMT',\n",
    "                  'cP',\n",
    "                  'cD1'           , 'cD2']\n",
    "\n",
    "nvar = len(variables2Plot)\n",
    "npcol = 4\n",
    "nprow = np.ceil(nvar / npcol)\n",
    "\n",
    "plt.figure(figsize=(20,5 * nprow))\n",
    "def plotVar(var1, pn):\n",
    "    plt.subplot(npcol, nprow, pn)\n",
    "    param = vals[var1]\n",
    "    \n",
    "    hist, bins = np.histogram(param, bins=50)\n",
    "    hist = 100.0 * hist / np.sum(hist)\n",
    "    bins = bins[1:] - np.diff(bins)/2\n",
    "    plt.plot(bins, hist)\n",
    "    plt.xlabel(var1)\n",
    "    \n",
    "pn = 0\n",
    "for i in variables2Plot:\n",
    "    pn = pn + 1\n",
    "    plotVar(i, pn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what to the sigmoids look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb import set_trace as browser\n",
    "\n",
    "def pltVsFire(x, xlab, pnt = 'o', *args, **kw):\n",
    "    plt.plot(x, fd.fire, pnt, alpha = 0.03, *args, **kw)\n",
    "    plt.xlabel(xlab)\n",
    "    \n",
    "def np_sigmoid(x, k, x0):\n",
    "    \"\"\"\n",
    "    Sigmoid function to describe limitation using tensor\n",
    "    \"\"\"\n",
    "    return 1.0/(1.0 + np.exp(-k*(x - x0)))\n",
    "\n",
    "def returnSigmoid(x, k, x0):\n",
    "    return np_sigmoid(x, k, x0)\n",
    "    \n",
    "def meanParam(x, x0, k, kmult = 1.0):\n",
    "    x0 = np.mean(vals[x0])\n",
    "    k  = np.mean(vals[k]) * kmult\n",
    "\n",
    "    return returnSigmoid(x, k, x0)\n",
    "\n",
    "def randomParam(x, x0, k, kmult = 1.0, size = 100):\n",
    "    ps = np.random.choice(vals.shape[0], size = size, replace = False)\n",
    "    return [returnSigmoid(x, vals[k][i] * kmult, vals[x0][i]) for i in ps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "####################\n",
    "## Fuel           ##\n",
    "####################\n",
    "plt.subplot(2, 2, 1)\n",
    "## scatter plot\n",
    "fp = np.mean(vals['fuel_pw'])\n",
    "fg = np.mean(vals['fuel_pg'])\n",
    "\n",
    "f_fuel = fuel_load(fd[\"vegCover\"].values, fd[\"alphaMax\"].values, fp, fg)\n",
    "print(f_fuel.max())\n",
    "pltVsFire(f_fuel, \"NPP (g/m$^2$)\", 'go')\n",
    "\n",
    "## Line of best fit\n",
    "Fuel = np.arange(0, f_fuel.max(), 0.01)\n",
    "r_fuel = randomParam(Fuel, 'fuel_x0', 'fuel_k')\n",
    "for r in r_fuel: plt.plot(Fuel, r, 'k', alpha=.01)\n",
    "\n",
    "####################\n",
    "## Moisture       ##\n",
    "####################\n",
    "plt.subplot(2, 2, 2)\n",
    "## scatter plot\n",
    "cM = np.mean(vals['cM'])\n",
    "cMT = np.mean(vals['cMT'])\n",
    "f_moisture = moisture(fd[\"alpha\"].values, fd[\"emc\"].values, fd[\"treecover\"].values, cM, cMT)\n",
    "pltVsFire(f_moisture , \"Moisture = $\\\\alpha$ + M $\\cdot$ EMC\",'bo')\n",
    "\n",
    "## Line of best fit\n",
    "mst = np.arange(0.0, f_moisture.max(), 0.05)\n",
    "r_moisture = randomParam(mst, 'moisture_x0', 'moisture_k', -1)\n",
    "for r in r_moisture: plt.plot(mst, r, 'k', alpha=.01)\n",
    "\n",
    "\n",
    "####################\n",
    "## Igntions       ##\n",
    "####################\n",
    "plt.subplot(2, 2, 3)\n",
    "## scatter plot \n",
    "cP  = np.mean(vals['cP' ])\n",
    "cD1 = np.mean(vals['cD1'])\n",
    "igniteMax = 10\n",
    "\n",
    "f_ignite = ignition(fd[\"lightning_ignitions\"].values, \\\n",
    "                    fd[\"pasture\"].values, \\\n",
    "                    fd[\"population_density\"].values, \\\n",
    "                    cP, cD1)\n",
    "\n",
    "pltVsFire(f_ignite, \"Ignitions events = Lightn + P $\\cdot$ Pop Dens + D1 $\\cdot$ Pasture\")\n",
    "\n",
    "## Line of best fit\n",
    "Ignite = np.arange(0.0, igniteMax, 0.1)\n",
    "r_Ignite = randomParam(Ignite, 'igntions_x0', 'igntions_k')\n",
    "for r in r_Ignite: plt.plot(Ignite, r, 'k', alpha=.01)\n",
    "\n",
    "plt.xlim(0, igniteMax)\n",
    "\n",
    "####################\n",
    "## Suppression    ##\n",
    "####################\n",
    "plt.subplot(2, 2, 4)\n",
    "#scatter plot\n",
    "cD2 = np.mean(vals['cD2'])\n",
    "f_suppression = supression(fd[\"cropland\"].values, \\\n",
    "                           fd[\"population_density\"].values, \\\n",
    "                           cD2)\n",
    "\n",
    "pltVsFire(f_suppression, \"Suppression = Cropland + D2 $\\cdot$ Pop den\")\n",
    "\n",
    "# Line of best fit\n",
    "Suppress = np.arange(0, 100, 0.01)\n",
    "r_suppression = randomParam(Suppress, 'suppression_x0', 'suppression_k', -1.0)\n",
    "for r in r_suppression: plt.plot(Suppress, r, 'k', alpha=.01)\n",
    "\n",
    "plt.xlim(0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the posterior for all data points\n",
    "We now need to run the model over all datapoints and all posterior solution. We do this by boostrapping the posterior n_posterior_sample times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_posterior = vals.shape[1]\n",
    "fd = pd.read_csv(DATAPATH)\n",
    "fd['vegCover'] = (100.0 - fd['bareground'])/100.0\n",
    "fd['alphaMax'] = fd['alpha_'] - 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the MCMC already introduces the required randomness for bootstrapping, we can just sample it over a simple sequance. Below, we claculate controls, limitations imposed by controls, some point soon control sensitivity, and overall burnt area for each i in the sample sequance. This is outputted into a csv file. Maybe some point I'll get it to output into a netcdf as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngap = int(n_posterior/n_posterior_sample)\n",
    "for i in range(0, n_posterior, ngap):\n",
    "    print(i)\n",
    "    f_fuel     = fuel_load(fd[\"vegCover\"].values, fd[\"alphaMax\"].values, vals['fuel_pw'][i], vals['fuel_pg'][i])\n",
    "    \n",
    "    f_moisture = moisture(fd[\"alpha\"].values, fd[\"emc\"].values, fd[\"treecover\"].values, vals['cM'][i], vals['cMT'][i])\n",
    "    f_ignition = ignition(fd[\"lightning_ignitions\"].values, \\\n",
    "                    fd[\"pasture\"].values, \\\n",
    "                    fd[\"population_density\"].values, \\\n",
    "                    vals['cP'][i], vals['cD1'][i])\n",
    "    f_suppression = supression(fd[\"cropland\"].values, \\\n",
    "                               fd[\"population_density\"].values, \\\n",
    "                               vals['cD2'][i])\n",
    "    \n",
    "    l_fuel        = np_sigmoid(f_fuel, vals['fuel_k'][i], vals['fuel_x0'][i])\n",
    "    l_moisture    = np_sigmoid(f_moisture, -vals['moisture_k'][i], vals['moisture_x0'][i])\n",
    "    l_ignitions   = np_sigmoid(f_ignition, vals['igntions_k'][i], vals['igntions_x0'][i])\n",
    "    l_suppression = np_sigmoid(f_suppression, -vals['suppression_k'][i], vals['suppression_x0'][i])\n",
    "    prediction = vals['max_f'][i] * np.product([l_fuel, l_moisture, l_ignitions, l_suppression])\n",
    "    \n",
    "    PS_outFile = '../outputs/sampled_posterior_ConFire_solutions/sample_no_' + str(i) +'.csv'\n",
    "    \n",
    "    out = pd.DataFrame({'fuel_continuity': f_fuel,\n",
    "                        'moisture_content': f_moisture,\n",
    "                        'ignitions': f_ignition,\n",
    "                        'suppression_index': f_suppression,\n",
    "                        'fuel_limitation': l_fuel,\n",
    "                        'moisture_limitation': l_moisture,\n",
    "                        'ignitions_limitation': l_ignitions,\n",
    "                        'suppression_limitation': l_suppression,\n",
    "                        'burnt_area': prediction})\n",
    "    out.to_csv(PS_outFile, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
